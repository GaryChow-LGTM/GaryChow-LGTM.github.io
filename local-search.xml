<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Flink FileSystemTableSink Code Intro</title>
    <link href="/2025/07/14/Flink-FileSystemTableSink-Code-Intro/"/>
    <url>/2025/07/14/Flink-FileSystemTableSink-Code-Intro/</url>
    
    <content type="html"><![CDATA[<h1 id="源码分析-Flink-FileSystemTableSink-中自定义分区策略的创建与调用机制详解"><a href="#源码分析-Flink-FileSystemTableSink-中自定义分区策略的创建与调用机制详解" class="headerlink" title="源码分析 - Flink FileSystemTableSink 中自定义分区策略的创建与调用机制详解"></a>源码分析 - Flink FileSystemTableSink 中自定义分区策略的创建与调用机制详解</h1><p>在使用 Flink 通过 Table API 或 SQL 写入文件系统时，若启用了自定义分区，系统如何注入并应用参数 sink.partition-commit.policy.kind 来控制分区提交行为？本文将从源码角度梳理这套机制的调用路径与实现细节。</p><p>本人实现的 [FLINK-32388]Add the ability to pass parameters to CUSTOM PartitionCommitPolicy <a href="https://github.com/apache/flink/pull/22831">https://github.com/apache/flink/pull/22831</a> 也是基于本篇文章的源码分析而实现的。</p><p>示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs java"># 配置双策略链（元数据提交+自定义）<br>sink.partition-commit.policy.kind: custom<br><br># 自定义策略配置<br>sink.partition-commit.policy.custom.class: com.company.CustomPolicy<br>sink.partition-commit.policy.custom.parameters: param1,param2<br></code></pre></td></tr></table></figure><hr><h2 id="一、调用链概览：参数作用于哪里？"><a href="#一、调用链概览：参数作用于哪里？" class="headerlink" title="一、调用链概览：参数作用于哪里？"></a><strong>一、调用链概览：参数作用于哪里？</strong></h2><p>当用户提交 Flink SQL 作业，FileSystemTableSink 会作为 TableSink 参与物理执行计划的构建。其核心的参数处理逻辑发生在如下调用链中：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">translateToPlanInternal</span><br> └── createSinkTransformation<br>     └── getSinkRuntimeProvider<br>         └── consumeDataStream<br>             └── consume<br>                 └── createStreamingSink<br>                     └── sink（创建 PartitionCommitter）<br>                         └── initializeState（构建策略链）<br></code></pre></td></tr></table></figure><hr><h2 id="二、物理计划转换：从-Plan-到-Sink-的转化"><a href="#二、物理计划转换：从-Plan-到-Sink-的转化" class="headerlink" title="二、物理计划转换：从 Plan 到 Sink 的转化"></a><strong>二、物理计划转换：从 Plan 到 Sink 的转化</strong></h2><p>Flink Planner 在构造物理执行计划时（Transformation 或 DataStream），会调用 TableSink 的 translateToPlanInternal 方法。在流式写入文件系统时，最终会进入 consume 方法中：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Override</span><br><span class="hljs-keyword">public</span> SinkRuntimeProvider <span class="hljs-title function_">getSinkRuntimeProvider</span><span class="hljs-params">(Context sinkContext)</span> &#123;<br>    <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">DataStreamSinkProvider</span>() &#123;<br>        <span class="hljs-meta">@Override</span><br>        <span class="hljs-keyword">public</span> DataStreamSink&lt;?&gt; consumeDataStream(<br>                ProviderContext providerContext, DataStream&lt;RowData&gt; dataStream) &#123;<br>            <span class="hljs-keyword">return</span> consume(providerContext, dataStream, sinkContext);<br>        &#125;<br>    &#125;;<br>&#125;<br></code></pre></td></tr></table></figure><p>通过”数据是否有界”来判断是否是流处理（unbounded）：</p><ul><li>若为流模式，则进入 createStreamingSink，构建 FileSystemSink；</li><li>否则使用 createBatchSink 执行批量输出。</li></ul><hr><h2 id="三、策略链构建核心：createStreamingSink-sink"><a href="#三、策略链构建核心：createStreamingSink-sink" class="headerlink" title="三、策略链构建核心：createStreamingSink -&gt; sink"></a><strong>三、策略链构建核心：createStreamingSink -&gt; sink</strong></h2><p>createStreamingSink 中最关键的调用是 sink() 方法，负责将数据写入文件系统并执行分区提交策略，保证数据一致性和可见性。</p><p>它负责最终创建分区提交组件 PartitionCommitter，并将其与 writerStream 进行连接：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> DataStreamSink&lt;?&gt; sink(<br>    ProviderContext providerContext,<br>    DataStream&lt;PartitionCommitInfo&gt; writerStream,<br>    Path path,<br>    ObjectIdentifier tableIdentifier,<br>    List&lt;String&gt; partitionKeys,<br>    TableMetaStoreFactory metaStoreFactory,<br>    FileSystemFactory fsFactory,<br>    ReadableConfig tableOptions<br>) &#123;<br>    <span class="hljs-comment">// 参数判断：是否启用分区提交策略</span><br>    <span class="hljs-keyword">if</span> (partitionKeys.size() &gt; <span class="hljs-number">0</span> &amp;&amp; options.contains(SINK_PARTITION_COMMIT_POLICY_KIND)) &#123;<br>        <span class="hljs-comment">// 构建提交器</span><br>        <span class="hljs-type">PartitionCommitter</span> <span class="hljs-variable">committer</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">PartitionCommitter</span>(<br>            path, tableIdentifier, partitionKeys, metaStoreFactory, fsFactory, tableOptions.toConfiguration()<br>        );<br>        <span class="hljs-keyword">return</span> writerStream.transform(<span class="hljs-string">&quot;Partition Committer&quot;</span>, committer);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><hr><h2 id="四、运行时策略链构建机制-PartitionCommitter"><a href="#四、运行时策略链构建机制-PartitionCommitter" class="headerlink" title="四、运行时策略链构建机制: PartitionCommitter"></a><strong>四、运行时策略链构建机制: PartitionCommitter</strong></h2><p>在 Flink Job 启动或恢复时，PartitionCommitter.initializeState() 被自动调用：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">initializeState</span><span class="hljs-params">(StateInitializationContext context)</span> <span class="hljs-keyword">throws</span> Exception &#123;<br>    <span class="hljs-type">PartitionCommitPolicyFactory</span> <span class="hljs-variable">factory</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">PartitionCommitPolicyFactory</span>(<br>        conf.get(SINK_PARTITION_COMMIT_POLICY_KIND),     <span class="hljs-comment">// 如 &quot;metastore,success-file&quot;</span><br>        conf.get(SINK_PARTITION_COMMIT_POLICY_CLASS),     <span class="hljs-comment">// 自定义类名（可选）</span><br>        conf.get(SINK_PARTITION_COMMIT_SUCCESS_FILE_NAME), <span class="hljs-comment">// success-file 策略用</span><br>        conf.get(SINK_PARTITION_COMMIT_POLICY_CLASS_PARAMETERS) <span class="hljs-comment">// 自定义类参数（可选）</span><br>    );<br>    <span class="hljs-built_in">this</span>.policies = factory.createPolicyChain(<br>        getUserCodeClassloader(), <span class="hljs-comment">// 用户类加载器</span><br>        () -&gt; &#123; <span class="hljs-comment">// 获取文件系统实例的工厂</span><br>            <span class="hljs-keyword">try</span> &#123;<br>                <span class="hljs-keyword">return</span> fsFactory.create(locationPath.toUri());<br>            &#125; <span class="hljs-keyword">catch</span> (IOException e) &#123;<br>                <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">RuntimeException</span>(e);<br>            &#125;<br>        &#125;<br>    );<br>&#125;<br></code></pre></td></tr></table></figure><hr><h2 id="五、策略工厂-PartitionCommitPolicyFactory-的核心逻辑"><a href="#五、策略工厂-PartitionCommitPolicyFactory-的核心逻辑" class="headerlink" title="五、策略工厂 PartitionCommitPolicyFactory 的核心逻辑"></a><strong>五、策略工厂 PartitionCommitPolicyFactory 的核心逻辑</strong></h2><p>策略链的创建会用到用户自定义类的类加载器(UserCodeClassloader)和文件系统工厂(FileSystem factory)，确保策略能够正确加载用户代码和操作分区文件系统。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">/** Create a policy chain. */</span><br><span class="hljs-keyword">public</span> List&lt;PartitionCommitPolicy&gt; <span class="hljs-title function_">createPolicyChain</span><span class="hljs-params">(</span><br><span class="hljs-params">        ClassLoader cl, Supplier&lt;FileSystem&gt; fsSupplier)</span> &#123;<br>    String[] policyStrings = policyKind.split(<span class="hljs-string">&quot;,&quot;</span>);<br>    <span class="hljs-keyword">return</span> Arrays.stream(policyStrings)<br>            .map(<br>                    name -&gt; &#123;<br>                        <span class="hljs-keyword">switch</span> (name.toLowerCase()) &#123;<br>                            <span class="hljs-keyword">case</span> PartitionCommitPolicy.METASTORE:<br>                                <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">MetastoreCommitPolicy</span>();<br>                            <span class="hljs-keyword">case</span> PartitionCommitPolicy.SUCCESS_FILE:<br>                                <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">SuccessFileCommitPolicy</span>(<br>                                        successFileName, fsSupplier.get());<br>                            <span class="hljs-keyword">case</span> PartitionCommitPolicy.CUSTOM:<br>                                <span class="hljs-keyword">try</span> &#123;<br>                                    <span class="hljs-keyword">if</span> (parameters != <span class="hljs-literal">null</span> &amp;&amp; !parameters.isEmpty()) &#123;<br>                                        String[] paramStrings =<br>                                                parameters.toArray(<span class="hljs-keyword">new</span> <span class="hljs-title class_">String</span>[<span class="hljs-number">0</span>]);<br>                                        Class&lt;?&gt;[] classes = <span class="hljs-keyword">new</span> <span class="hljs-title class_">Class</span>&lt;?&gt;[parameters.size()];<br>                                        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; i &lt; parameters.size(); i++) &#123;<br>                                            classes[i] = String.class;<br>                                        &#125;<br>                                        <span class="hljs-keyword">return</span> (PartitionCommitPolicy)<br>                                                cl.loadClass(customClass)<br>                                                        .getConstructor(classes)<br>                                                        .newInstance((Object[]) paramStrings);<br>                                    &#125; <span class="hljs-keyword">else</span> &#123;<br>                                        <span class="hljs-keyword">return</span> (PartitionCommitPolicy)<br>                                                cl.loadClass(customClass).newInstance();<br>                                    &#125;&#125;&#125;&#125;&#125;)<br>            .collect(Collectors.toList());&#125;<br>&#125;<br></code></pre></td></tr></table></figure><blockquote><p>💡 关键点：自定义类必须实现 PartitionCommitPolicy 接口，并提供合适的构造方法（带参或无参均可）。</p></blockquote><hr><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><p>参数如 <code>sink.partition-commit.policy.kind = metastore,success-file</code> 的注入生效流程如下：</p><figure class="highlight isbl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs isbl">用户配置<br>  ↓<br><span class="hljs-variable">FileSystemTableSink</span> -&gt; <span class="hljs-function"><span class="hljs-title">sink</span>()</span><br>  ↓<br>构造 <span class="hljs-variable">PartitionCommitter</span><br>  ↓<br><span class="hljs-function"><span class="hljs-title">initializeState</span>()</span><br>  ↓<br><span class="hljs-variable">PartitionCommitPolicyFactory</span> -&gt; <span class="hljs-function"><span class="hljs-title">createPolicyChain</span>()</span><br>  ↓<br>创建策略实例 → 构成策略链 → 实现 <span class="hljs-variable">PartitionCommitter</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>CodeDive</tag>
      
      <tag>Apache Flink</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ingress and Route53 Issues with Druid AWS Deployments</title>
    <link href="/2025/03/19/ingress-and-route53-issues-with-druid-aws-deployments/"/>
    <url>/2025/03/19/ingress-and-route53-issues-with-druid-aws-deployments/</url>
    
    <content type="html"><![CDATA[<h1 id="Ingress-and-Route53-Issues-with-Druid-AWS-Deployments"><a href="#Ingress-and-Route53-Issues-with-Druid-AWS-Deployments" class="headerlink" title="Ingress and Route53 Issues with Druid AWS Deployments"></a>Ingress and Route53 Issues with Druid AWS Deployments</h1><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p> When deploying Druid with Kubernetes on AWS, it is common to use AWS ALB (Application Load Balancer) for traffic routing and load balancing. By configuring Kubernetes’ Ingress resource, ALB can route traffic to different services. At the same time, Route 53 needs to be configured to handle DNS domain name resolution so that applications can be accessed via domain names.</p><p> This configuration example shows how to set up the AWS ALB Ingress Controller to associate with the Kubernetes Ingress resource.</p><h2 id="Debug"><a href="#Debug" class="headerlink" title="Debug"></a>Debug</h2><ol><li><p><strong>Obtaining the ALB Certificate ARN</strong>: If you need to configure an SSL&#x2F;TLS certificate for ALB, you can obtain the certificate ARN through AWS ACM (AWS Certificate Manager), which <strong>you need to request from the SRE team</strong>. This certificate is used for HTTPS configuration. The following is a sample configuration:</p> <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">ingress:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">alb-ingress-route</span><br>  <span class="hljs-attr">domain:</span> <span class="hljs-string">druid-1.net</span><br>  <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span><br><br><span class="hljs-attr">ingress2:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">alb-ingress2-route</span><br>  <span class="hljs-attr">domain:</span> <span class="hljs-string">druid-2.net</span><br>  <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span><br><br><span class="hljs-attr">ingressClass:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">alb</span><br>  <span class="hljs-attr">annotations:</span><br>    <span class="hljs-attr">alb.ingress.kubernetes.io/certificate-arn:</span> <span class="hljs-string">arn:aws:acm:xxx</span><br>    <span class="hljs-attr">alb.ingress.kubernetes.io/subnets:</span> <span class="hljs-string">sub-xxx</span><br>    <span class="hljs-attr">alb.ingress.kubernetes.io/group.name:</span> <span class="hljs-string">druid</span><br>    <span class="hljs-attr">alb.ingress.kubernetes.io/healthcheck-path:</span> <span class="hljs-string">/status/health</span><br>    <span class="hljs-attr">alb.ingress.kubernetes.io/healthcheck-port:</span> <span class="hljs-string">&quot;8888&quot;</span><br>    <span class="hljs-attr">alb.ingress.kubernetes.io/listen-ports:</span> <span class="hljs-string">&#x27;[&#123;&quot;HTTP&quot;: 80&#125;, &#123;&quot;HTTPS&quot;: 443&#125;]&#x27;</span><br>    <span class="hljs-attr">alb.ingress.kubernetes.io/manage-backend-security-group-rules:</span> <span class="hljs-string">&quot;true&quot;</span><br>    <span class="hljs-attr">alb.ingress.kubernetes.io/scheme:</span> <span class="hljs-string">internal</span><br>    <span class="hljs-attr">alb.ingress.kubernetes.io/success-codes:</span> <span class="hljs-string">&quot;200&quot;</span><br>    <span class="hljs-attr">alb.ingress.kubernetes.io/tags:</span> <span class="hljs-string">Owner=Gary</span><br>    <span class="hljs-attr">alb.ingress.kubernetes.io/target-type:</span> <span class="hljs-string">ip</span><br><br></code></pre></td></tr></table></figure></li><li><p><strong>Get Kubernetes Ingress address</strong>: Query the Ingress resource in Kubernetes via <code>kubectl</code> to get the ALB address. The following is a sample command and output:</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">bwdev get ingress<br>NAME                      CLASS   HOSTS                               ADDRESS                                                                  PORTS   AGE<br>alb-ingress-route         alb     druid-1.net          internal-k8s-druid-xxx.elb.amazonaws.com   80      31d<br>alb-ingress2-route        alb     druid-2.net   internal-k8s-druid-xxx.elb.amazonaws.com   80      31d<br></code></pre></td></tr></table></figure><p>  The <code>ADDRESS</code> field is the DNS address of the AWS ALB. Use it as a record for configuring Route 53.</p></li><li><p><strong>Modify the AWS Route 53 resource</strong>: Configure the appropriate A record or CNAME record for your domain name in AWS Route 53, ensuring that the <code>druid-1.net</code> and <code>druid-2.net</code> domain names point to the DNS address of the ALB (for example: <code>internal-k8s-druid-xxx.elb.amazonaws.com</code> ).</p><p>  In the AWS Route 53 console, perform the following steps:</p><ul><li>Create or update an A record that points to the DNS address of the ALB.</li><li>Ensure that the domain name resolution is set to Public or Private as required.</li></ul><p>  Example record:</p> <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-string">druid-1.net</span>   <span class="hljs-string">A</span>   <span class="hljs-string">internal-k8s-druid-xxx.elb.amazonaws.com</span><br><span class="hljs-string">druid-2.net</span>   <span class="hljs-string">A</span>   <span class="hljs-string">internal-k8s-druid-xxx.elb.amazonaws.com</span><br></code></pre></td></tr></table></figure></li><li><p><strong>Verify Configuration</strong>: After completing the ALB and Route 53 configuration, you can verify that the ALB is routing traffic properly by accessing domain names such as <code>druid-1.net</code> and <code>druid-2.net</code>.</p></li></ol><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p> With the ALB Ingress Controller and Route 53 properly configured, you can route traffic to services in Kubernetes and use AWS-provided certificates for HTTPS encryption. The key steps include obtaining the certificate ARN, querying the Ingress address, and updating the Route 53 configuration. Ensure that all resources and domains are configured correctly to access your application.</p><hr><h1 id="Druid-AWS部署的ingress和route53问题"><a href="#Druid-AWS部署的ingress和route53问题" class="headerlink" title="Druid AWS部署的ingress和route53问题"></a>Druid AWS部署的ingress和route53问题</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在 AWS 上使用 Kubernetes 部署 Druid 时，通常会使用 AWS ALB（Application Load Balancer）来进行流量的路由和负载均衡。通过配置 Kubernetes 的 Ingress 资源，ALB 可以将流量路由到不同的服务。与此同时，还需要配置 Route 53 来处理 DNS 域名解析，以便通过域名访问应用。</p><p>该配置示例展示了如何设置 AWS ALB Ingress Controller 与 Kubernetes Ingress 资源的关联。</p><h2 id="调试"><a href="#调试" class="headerlink" title="调试"></a>调试</h2><ol><li><p><strong>获取 ALB 证书 ARN</strong>：如果需要为 ALB 配置 SSL&#x2F;TLS 证书，可以通过 AWS ACM（AWS Certificate Manager）获得证书 ARN。<strong>需要向 SRE 团队请求该证书 ARN</strong>。这个证书用于 HTTPS 配置。以下是示例配置：</p> <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">ingress:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">alb-ingress-route</span><br>  <span class="hljs-attr">domain:</span> <span class="hljs-string">druid-1.net</span><br>  <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span><br><br><span class="hljs-attr">ingress2:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">alb-ingress2-route</span><br>  <span class="hljs-attr">domain:</span> <span class="hljs-string">druid-2.net</span><br>  <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span><br><br><span class="hljs-attr">ingressClass:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">alb</span><br>  <span class="hljs-attr">annotations:</span><br>    <span class="hljs-attr">alb.ingress.kubernetes.io/certificate-arn:</span> <span class="hljs-string">arn:aws:acm:xxx</span><br>    <span class="hljs-attr">alb.ingress.kubernetes.io/subnets:</span> <span class="hljs-string">sub-xxx</span><br>    <span class="hljs-attr">alb.ingress.kubernetes.io/group.name:</span> <span class="hljs-string">druid</span><br>    <span class="hljs-attr">alb.ingress.kubernetes.io/healthcheck-path:</span> <span class="hljs-string">/status/health</span><br>    <span class="hljs-attr">alb.ingress.kubernetes.io/healthcheck-port:</span> <span class="hljs-string">&quot;8888&quot;</span><br>    <span class="hljs-attr">alb.ingress.kubernetes.io/listen-ports:</span> <span class="hljs-string">&#x27;[&#123;&quot;HTTP&quot;: 80&#125;, &#123;&quot;HTTPS&quot;: 443&#125;]&#x27;</span><br>    <span class="hljs-attr">alb.ingress.kubernetes.io/manage-backend-security-group-rules:</span> <span class="hljs-string">&quot;true&quot;</span><br>    <span class="hljs-attr">alb.ingress.kubernetes.io/scheme:</span> <span class="hljs-string">internal</span><br>    <span class="hljs-attr">alb.ingress.kubernetes.io/success-codes:</span> <span class="hljs-string">&quot;200&quot;</span><br>    <span class="hljs-attr">alb.ingress.kubernetes.io/tags:</span> <span class="hljs-string">Owner=Gary</span><br>    <span class="hljs-attr">alb.ingress.kubernetes.io/target-type:</span> <span class="hljs-string">ip</span><br></code></pre></td></tr></table></figure></li><li><p><strong>获取 Kubernetes Ingress 地址</strong>：通过 <code>kubectl</code> 查询 Kubernetes 中的 Ingress 资源，获取 ALB 地址。以下是命令和输出示例：</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">bwdev get ingress<br>NAME                      CLASS   HOSTS                               ADDRESS                                                                  PORTS   AGE<br>alb-ingress-route         alb     druid-1.net          internal-k8s-druid-xxx.elb.amazonaws.com   80      31d<br>alb-ingress2-route        alb     druid-2.net   internal-k8s-druid-xxx.elb.amazonaws.com   80      31d<br></code></pre></td></tr></table></figure><p> <code>ADDRESS</code> 字段即为 AWS ALB 的 DNS 地址。将其用作配置 Route 53 的记录。</p></li><li><p><strong>修改 AWS Route 53 资源</strong>：在 AWS Route 53 中为您的域名配置相应的 A 记录或 CNAME 记录，确保 <code>druid-1.net</code> 和 <code>druid-2.net</code> 域名指向 ALB 的 DNS 地址（例如：<code>internal-k8s-druid-xxx.elb.amazonaws.com</code>）。</p><p> 在 AWS Route 53 控制台中，执行以下步骤：</p><ul><li>创建或更新 A 记录，指向 ALB 的 DNS 地址。</li><li>确保域名解析设置为公共或私有根据需求。</li></ul><p> 示例记录：</p> <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-string">druid-1.net</span>   <span class="hljs-string">A</span>   <span class="hljs-string">internal-k8s-druid-xxx.elb.amazonaws.com</span><br><span class="hljs-string">druid-2.net</span>   <span class="hljs-string">A</span>   <span class="hljs-string">internal-k8s-druid-xxx.elb.amazonaws.com</span><br></code></pre></td></tr></table></figure></li><li><p><strong>验证配置</strong>：完成 ALB 和 Route 53 配置后，您可以通过访问域名（如 <code>druid-1.net</code> 和 <code>druid-2.net</code>）验证 ALB 是否正常路由流量。</p></li></ol><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>通过正确配置 ALB Ingress Controller 和 Route 53，您可以将流量路由到 Kubernetes 中的服务，并且使用 AWS 提供的证书进行 HTTPS 加密。关键的步骤包括获取证书 ARN、查询 Ingress 地址以及更新 Route 53 配置。确保所有资源和域名配置正确，以便访问您的应用程序。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Debug</tag>
      
      <tag>Druid</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Frame Size Configuration and RowTooLarge Error Resolution in Druid</title>
    <link href="/2025/03/19/Frame-Size-Configuration-and-RowTooLarge-Error-Resolution-in-Druid/"/>
    <url>/2025/03/19/Frame-Size-Configuration-and-RowTooLarge-Error-Resolution-in-Druid/</url>
    
    <content type="html"><![CDATA[<h1 id="Frame-Size-Configuration-and-RowTooLarge-Error-Resolution-in-Druid"><a href="#Frame-Size-Configuration-and-RowTooLarge-Error-Resolution-in-Druid" class="headerlink" title="Frame Size Configuration and RowTooLarge Error Resolution in Druid"></a>Frame Size Configuration and RowTooLarge Error Resolution in Druid</h1><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p> In Apache Druid, data processing usually involves dividing large-scale data into multiple “frames” for processing. In this process, the frame size is a key parameter that determines how much data can be processed in each frame. A frame size that is too large or too small can affect performance or cause processing to fail. This error log indicates that while processing a particular piece of data, a row of data was encountered whose size exceeded the specified frame size, causing the task to fail.</p><p> Error Message:</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">RowTooLarge</span>: Encountered row that cannot fit in a single frame (max frame size = <span class="hljs-number">1</span>,<span class="hljs-number">000</span>,<span class="hljs-number">000</span>)<br></code></pre></td></tr></table></figure><p> Indicates that while processing data, Druid encountered a row of data whose size exceeded <code>1,000,000</code> bytes (i.e., <code>STANDARD_FRAME_SIZE</code> ) and was unable to fit the current frame size limit.</p><h2 id="Debug"><a href="#Debug" class="headerlink" title="Debug"></a>Debug</h2><ol><li><p><strong>Frame Size Configuration</strong>: As you can see from the code snippet, Druid’s default frame size is <code>1,000,000</code> bytes. the value of <code>STANDARD_FRAME_SIZE</code> is statically defined:</p> <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">int</span> <span class="hljs-variable">STANDARD_FRAME_SIZE</span> <span class="hljs-operator">=</span> <span class="hljs-number">1_000_000</span>;<br></code></pre></td></tr></table></figure><p>  This value is used as the standard frame size for memory allocation during Druid task execution:</p> <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs java">  <span class="hljs-keyword">final</span> <span class="hljs-type">InputChannels</span> <span class="hljs-variable">inputChannels</span> <span class="hljs-operator">=</span><br>      <span class="hljs-keyword">new</span> <span class="hljs-title class_">InputChannelsImpl</span>(<br>          workOrder.getQueryDefinition(),<br>          InputSlices.allReadablePartitions(workOrder.getInputs()),<br>          inputChannelFactory,<br>          () -&gt; ArenaMemoryAllocator.createOnHeap(frameContext.memoryParameters().getStandardFrameSize()),<br>          exec,<br>          cancellationId<br>      );<br>    <br><span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">getStandardFrameSize</span><span class="hljs-params">()</span><br>&#123;<br>  <span class="hljs-keyword">return</span> STANDARD_FRAME_SIZE;<br>&#125;<br></code></pre></td></tr></table></figure></li><li><p><strong>Task Failure Reason</strong>: The task failed because a row of data exceeded the <code>1,000,000</code> byte size limit and could not fit into a single frame. This error message indicates that during execution, a row of data was too large for the set maximum frame size, causing the task to fail.</p></li><li><p><strong>Modifying the frame size</strong>: If larger rows of data need to be processed, consider increasing the size of <code>STANDARD_FRAME_SIZE</code> to allow larger rows of data to be processed per frame. For example, this could be modified to <code>2,000,000</code> or higher:</p> <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">int</span> <span class="hljs-variable">STANDARD_FRAME_SIZE</span> <span class="hljs-operator">=</span> <span class="hljs-number">2_000_000</span>;<br></code></pre></td></tr></table></figure></li><li><p><strong>Memory Optimization</strong>: Increasing the frame size may increase memory consumption, so there is a trade-off between memory usage and data processing performance. If there is not enough memory, increasing the frame size may cause problems such as memory overflow. When adjusting this value, you need to consider the overall system memory and load.</p></li></ol><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p> The task failed because a row of data exceeded the maximum frame size ( <code>1,000,000</code> bytes) configured by Druid. To resolve this issue, you can consider the following options:</p><ol><li><strong>Increase the frame size</strong>: Increase <code>STANDARD_FRAME_SIZE</code> to a larger value, such as <code>2,000,000</code> or higher, to be able to handle larger rows of data.</li><li><strong>Optimize data row size</strong>: If possible, adjust how data is generated to reduce the size of a single row of data.</li><li><strong>Memory management</strong>: Increasing the frame size may increase memory consumption and you need to make sure that the system has enough memory available.</li></ol><p> When adjusting the frame size, it is recommended to test it in conjunction with actual memory usage to avoid other problems caused by insufficient memory.</p><hr><h1 id="Druid-中帧大小配置及-RowTooLarge-错误解决方案"><a href="#Druid-中帧大小配置及-RowTooLarge-错误解决方案" class="headerlink" title="Druid 中帧大小配置及 RowTooLarge 错误解决方案"></a>Druid 中帧大小配置及 RowTooLarge 错误解决方案</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在 Apache Druid 中，数据处理通常涉及将大规模数据分为多个“帧”（frame）进行处理。在这个过程中，帧大小（frame size）是一个关键参数，它决定了每个帧最多可以处理多少数据。帧大小过大或过小都可能影响性能或导致处理失败。该错误日志表明在处理某一数据时，遇到一个行数据大小超过了指定的帧大小，从而导致任务失败。</p><p>错误信息：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">RowTooLarge</span>: Encountered row that cannot fit in a single frame (max frame size = <span class="hljs-number">1</span>,<span class="hljs-number">000</span>,<span class="hljs-number">000</span>)<br></code></pre></td></tr></table></figure><p>表明在处理数据时，Druid 遇到了一个数据行，它的大小超过了 <code>1,000,000</code> 字节（即 <code>STANDARD_FRAME_SIZE</code>），无法适配当前的帧大小限制。</p><h2 id="调试"><a href="#调试" class="headerlink" title="调试"></a>调试</h2><ol><li><p><strong>帧大小配置</strong>：从代码片段可以看到，Druid 默认的帧大小是 <code>1,000,000</code> 字节。<code>STANDARD_FRAME_SIZE</code> 的值是静态定义的：</p> <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">int</span> <span class="hljs-variable">STANDARD_FRAME_SIZE</span> <span class="hljs-operator">=</span> <span class="hljs-number">1_000_000</span>;<br></code></pre></td></tr></table></figure><p> 该值在 Druid 任务执行过程中被用作内存分配的标准帧大小：</p> <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs java">  <span class="hljs-keyword">final</span> <span class="hljs-type">InputChannels</span> <span class="hljs-variable">inputChannels</span> <span class="hljs-operator">=</span><br>      <span class="hljs-keyword">new</span> <span class="hljs-title class_">InputChannelsImpl</span>(<br>          workOrder.getQueryDefinition(),<br>          InputSlices.allReadablePartitions(workOrder.getInputs()),<br>          inputChannelFactory,<br>          () -&gt; ArenaMemoryAllocator.createOnHeap(frameContext.memoryParameters().getStandardFrameSize()),<br>          exec,<br>          cancellationId<br>      );<br>    <br><span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">getStandardFrameSize</span><span class="hljs-params">()</span><br>&#123;<br>  <span class="hljs-keyword">return</span> STANDARD_FRAME_SIZE;<br>&#125;<br></code></pre></td></tr></table></figure></li><li><p><strong>任务失败原因</strong>：任务失败的原因是某个数据行的大小超过了 <code>1,000,000</code> 字节的限制，导致无法装入单一帧中。该错误信息表明，在执行过程中，某一行数据太大，超过了设置的帧最大大小，导致任务失败。</p></li><li><p><strong>修改帧大小</strong>：如果需要处理更大的数据行，可以考虑增加 <code>STANDARD_FRAME_SIZE</code> 的大小，从而允许每个帧处理更大的数据行。例如，可以将其修改为 <code>2,000,000</code> 或更高：</p> <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">int</span> <span class="hljs-variable">STANDARD_FRAME_SIZE</span> <span class="hljs-operator">=</span> <span class="hljs-number">2_000_000</span>;<br></code></pre></td></tr></table></figure></li><li><p><strong>内存优化</strong>：增加帧大小可能会增加内存消耗，因此需要权衡内存使用和数据处理性能。如果内存不足，增加帧大小可能会导致内存溢出等问题。调整该值时，需要考虑系统内存和负载的整体情况。</p></li></ol><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>任务失败是由于某一行数据超过了 Druid 配置的最大帧大小（<code>1,000,000</code> 字节）。为解决此问题，您可以考虑以下几种方案：</p><ol><li><strong>增加帧大小</strong>：将 <code>STANDARD_FRAME_SIZE</code> 增加到更大的值，如 <code>2,000,000</code> 或更高，以便能够处理更大的数据行。</li><li><strong>优化数据行大小</strong>：如果可能，调整数据的生成方式，减少单行数据的大小。</li><li><strong>内存管理</strong>：增加帧大小可能会增加内存消耗，需要确保系统有足够的可用内存。</li></ol><p>调整帧大小时，建议结合实际内存使用情况进行测试，避免因内存不足导致其他问题。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Debug</tag>
      
      <tag>Druid</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Spark HistoryServer Intros and Troubleshooting</title>
    <link href="/2024/07/31/spark-history-debug/"/>
    <url>/2024/07/31/spark-history-debug/</url>
    
    <content type="html"><![CDATA[<h2 id="HistoryServer-Introduction"><a href="#HistoryServer-Introduction" class="headerlink" title="HistoryServer Introduction"></a>HistoryServer Introduction</h2><p>HistoryServer is an HTTP service built into Spark, we can start it with the following code. When HistoryServer is started, it starts two threads to parse and clean the log files.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">./sbin/start-history-server.sh<br></code></pre></td></tr></table></figure><p>This creates a web interface at <code>http://&lt;server-url&gt;:8080</code>, listing incomplete and completed applications and attempts.</p><p>The spark jobs themselves must be configured to log events, and to log them to the same shared, writable directory. </p><p>For example, if the server was configured with a log directory of <code>hdfs://namenode/spark-logs</code>, then the client-side options would be:</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">spark<span class="hljs-selector-class">.eventLog</span><span class="hljs-selector-class">.enabled</span> true<br>spark<span class="hljs-selector-class">.eventLog</span><span class="hljs-selector-class">.dir</span> hdfs:<span class="hljs-comment">//namenode/spark-logs</span><br></code></pre></td></tr></table></figure><p><img src="/2024/07/31/spark-history-debug/img1.png" title="img1"></p><p>To introduce the Spark History Server, the flowchart below outlines its workflow.</p><p>The Spark History Server relies on the Spark Event system. During a Spark task, various SparkListenerEvents are generated, like ApplicationStart and StageCompleted. These events are sent to the ListenerBus, where they are monitored by registered listeners. The EventLoggingListener writes these events to a JSON log file on a file system like HDFS.</p><p>The FsHistoryProvider in the History Server scans the log storage path periodically, extracts profile information (such as Application_id, user, status, start_time, end_time, event_log_path) from the log files, and maintains a list of this data. When a user accesses the UI, the system searches this list for the requested task and reads the corresponding event log file for details if it exists.</p><h2 id="Issue"><a href="#Issue" class="headerlink" title="Issue"></a>Issue</h2><p>In some cases, after submitting a job using Spark, the log information corresponding to the appId cannot be found in the <strong>Spark HistoryServer WebUI (v3.2.1)</strong>. </p><p>As a result, users cannot analyze the job log through the HistoryServer (HS). Upon analyzing the source code, it was found that the issue is due to the misuse of a data structure in a multithreading context.</p><h2 id="Data-Structure"><a href="#Data-Structure" class="headerlink" title="Data Structure"></a>Data Structure</h2><p>Before explaining the problem, it’s important to understand the key data structure in HS. The misuse of this data structure causes the issue. HS uses a data structure called Listing to store key-value pairs, such as:</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs xml">&lt;appId, InfoData<span class="hljs-tag">&lt;<span class="hljs-name">appId</span>&gt;</span>&gt;<br>&lt;logPath, InfoData<span class="hljs-tag">&lt;<span class="hljs-name">logPath</span>&gt;</span>&gt;<br></code></pre></td></tr></table></figure><h2 id="Log-Parsing-Process"><a href="#Log-Parsing-Process" class="headerlink" title="Log Parsing Process"></a>Log Parsing Process</h2><p>The HS runs the <code>checkForLogs</code> method every 10 seconds by default to build the application list based on the current contents of the log directory. During each execution, this method scans the specified logDir and parses the log files to update its KVStore listing. The process includes two main methods:</p><ol><li>update&#x2F;add log file records respectively</li><li>remove stale log file records.</li></ol><p>These methods are executed in different threads via <code>submitLogProcessTask</code>.</p><h2 id="Issue-Scenario"><a href="#Issue-Scenario" class="headerlink" title="Issue Scenario"></a>Issue Scenario</h2><p>When <code>checkForLogs</code> is executed for the first time, the ABC job submitted by the user is still running. The file name in the log directory is <code>ABC.inprocess</code>, so the KVStore <code>Listing</code> adds two new entries:</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs 1c">&lt;&#x27;ABC&#x27;, InfoData&lt;&#x27;ABC&#x27;&gt;&gt;<br>&lt;&#x27;/tmp/ABC.inprocess&#x27;, InfoData&lt;&#x27;/tmp/ABC.inprocess&#x27;&gt;&gt;<br></code></pre></td></tr></table></figure><p>In the next run of <code>checkForLogs</code>, now the ABC application has finished, the log file <code>/tmp/ABC.inprocess</code> has been deleted, and the new log file path is <code>/tmp/ABC</code>, so in this run of <code>checkForLogs</code>, two threads will start and do following things:</p><p><strong>Thread 1</strong>: update&#x2F;add log file records, this thread will parsing the log path <code>/tmp/ABC</code>. First it will update the<code> &lt;&#39;ABC&#39;, InfoData&lt;&#39;ABC&#39;&gt;&gt;</code> in the KVstore Listing, second it will add a new record <code>&lt;&#39;/tmp/ABC&#39;, InfoData&lt;&#39;/tmp/ABC&#39;&gt;&gt;</code> in KVstore Listing.</p><p><strong>Thread 2</strong>: remove stale log file records, this thread will notice that the log path <code>/tmp/ABC.inprocess</code> has disappeared. First it will filter this path as a stale data, second it will start <code>cleanAppData</code> method, in this method the following code will be executed:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-type">val</span> <span class="hljs-variable">app</span> <span class="hljs-operator">=</span> load(appId)<br></code></pre></td></tr></table></figure><p>Usually, by the time this method loads the <code>appId</code>, Thread 1 has already updated <code>&lt;&#39;ABC&#39;, InfoData&lt;&#39;ABC&#39;&gt;&gt;</code> in the KVStore Listing. Therefore, when the following code executes, the value of <code>isStale</code> will be <code>false</code>.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-type">val</span> <span class="hljs-variable">isStale</span> <span class="hljs-operator">=</span> attempt.headOption.exists &#123; a =&gt;<br>   <span class="hljs-keyword">if</span> (a.logPath != <span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(logPath).getName()) &#123;<br>     <span class="hljs-literal">false</span><br>   &#125; <span class="hljs-keyword">else</span> &#123;<br>     ...<br>     <span class="hljs-literal">true</span><br>   &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>However, in some cases, when this method loads the <code>appId</code>, Thread 1 has not finished the update. As a result, when the code above executes, the value of <code>isStale</code> will be <code>true</code>. The worst outcome is that it will delete the log file record <code>&lt;&#39;ABC&#39;, InfoData&lt;&#39;ABC&#39;&gt;&gt;</code> in the KVStore Listing after Thread 1 has completed the update.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">if</span> (isStale) &#123;<br>   <span class="hljs-keyword">if</span> (others.nonEmpty) &#123;<br>     <span class="hljs-type">val</span> <span class="hljs-variable">newAppInfo</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">ApplicationInfoWrapper</span>(app.info, others)<br>     listing.write(newAppInfo)<br>   &#125; <span class="hljs-keyword">else</span> &#123;<br>     listing.delete(classOf[ApplicationInfoWrapper], appId)<br>   &#125;<br> &#125;<br>&#125; <span class="hljs-keyword">catch</span> &#123;<br>  <span class="hljs-keyword">case</span> _: NoSuchElementException =&gt;<br>&#125;<br></code></pre></td></tr></table></figure><p>So, the log file record <code>&lt;&#39;ABC&#39;, InfoData&lt;&#39;ABC&#39;&gt;&gt;</code> will be permanently removed. When users access the WebUI for this application, they will see “Application not found” even though the log file for the app still exists.</p><p>One thing to note is that the log file record <code>&lt;&#39;ABC&#39;, InfoData&lt;&#39;ABC&#39;&gt;&gt;</code> will not be added to the KVStore Listing again. In the next run of <code>checkForLogs</code>, the entry <code>&lt;&#39;/tmp/ABC&#39;, InfoData&lt;&#39;/tmp/ABC&#39;&gt;&gt;</code> indicates that the log file of ABC does not need to be reloaded, so the method will skip this log file.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-type">val</span> <span class="hljs-variable">info</span> <span class="hljs-operator">=</span> listing.read(classOf[LogInfo], reader.rootPath.toString())<br> <br><span class="hljs-keyword">if</span> (shouldReloadLog(info, reader)) &#123;<br>    ...<br>    &#125; <span class="hljs-keyword">else</span> &#123;<br>    <span class="hljs-literal">false</span><br>    &#125;<br></code></pre></td></tr></table></figure><p>To better show the process of bug occurrence, below is the flowchart</p><p><img src="/2024/07/31/spark-history-debug/img2.png" title="img2"></p><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><p>Due to multi-thread error common use of KVstore Listing, we make the <code>cleanAppData</code> method atomic, just like the addListing mothod, this allows threads to use KVstore Listing without data errors. </p><p>For example:</p><p>If Thread 1 gets the lock on the KVstore Listing before Thread 2, it will update the record of <code>&lt;appId, InfoData&lt;appId&gt;&gt;</code>, so Thread 2 can safely get the record and it will make isStale be <code>false</code>, so the <code>cleanAppData</code> method will not delete the record in KVstore Listing.</p><p>If Thread 2 gets the lock on the KVstore Listing before Thread 1, it will make isStale be <code>true</code>, and the <code>cleanAppData</code> method will delete the record in KVstore Listing. But it will be ok, beacuse Thread 1 will add the record of <code>&lt;appId, InfoData&lt;appId&gt;&gt;</code> in KVstore Listing again.</p><p>In all situations above, the record of KVstore Listing will not remove unexpectedly.</p><p>This is the method used to fix the problem in Spark v3.2.2 (<a href="https://github.com/apache/spark/pull/36424/files">https://github.com/apache/spark/pull/36424/files</a>).</p>]]></content>
    
    
    
    <tags>
      
      <tag>Debug</tag>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes nodeAffinity &amp; nodeSelector Intros</title>
    <link href="/2024/07/31/Kubernetes-nodeAffinity-And-Selector/"/>
    <url>/2024/07/31/Kubernetes-nodeAffinity-And-Selector/</url>
    
    <content type="html"><![CDATA[<h2 id="Kubernetes-的亲和性调度"><a href="#Kubernetes-的亲和性调度" class="headerlink" title="Kubernetes 的亲和性调度"></a>Kubernetes 的亲和性调度</h2><p>K8s的 POD 是自动调度选择某个 NODE 的，默认情况下调度器考虑的是资源足够，并且负载尽量平均，但是有的时候我们需要能够更加细粒度的去控制 POD 的调度，比如控制 POD 是否在同一 NODE 上。这就需要用到 Kubernetes 里面的一个概念：<strong>亲和性</strong>，亲和性主要分为两类：<code>nodeAffinity</code>和<code>podAffinity</code>。</p><h2 id="nodeSelector"><a href="#nodeSelector" class="headerlink" title="nodeSelector"></a>nodeSelector</h2><p>用户可以非常灵活的利用 <code>label</code> 来管理集群中的资源，比如最常见的一个就是 service 通过匹配 <code>label</code> 去选择 POD 的。而 POD 的调度也可以根据节点的 label 进行特定的部署。</p><p>我们可以通过下面的命令查看我们的 node 的 label：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kubectl get nodes --show-labels<br>NAME            STATUS    ROLES     AGE       VERSION   LABELS<br>192.168.1.140   Ready     &lt;none&gt;    42d       v1.8.1    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=192.168.1.140<br>192.168.1.161   Ready     &lt;none&gt;    118d      v1.8.1    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/cluster-service=<span class="hljs-literal">true</span>,kubernetes.io/hostname=192.168.1.161<br>192.168.1.170   Ready     &lt;none&gt;    118d      v1.8.1    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/cluster-service=<span class="hljs-literal">true</span>,kubernetes.io/hostname=192.168.1.170<br>192.168.1.172   Ready     &lt;none&gt;    114d      v1.8.1    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/cluster-service=<span class="hljs-literal">true</span>,kubernetes.io/hostname=192.168.1.172<br><br></code></pre></td></tr></table></figure><p>现在我们先给节点<strong>192.168.1.140</strong>增加一个<code>source=qikqiak</code>的标签，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kubectl label nodes 192.168.1.140 <span class="hljs-built_in">source</span>=qikqiak<br>node <span class="hljs-string">&quot;192.168.1.140&quot;</span> labeled<br><br></code></pre></td></tr></table></figure><p>我们可以通过上面的<code>--show-labels</code>参数可以查看上述标签是否生效。当 node 被打上了相关标签后，在调度的时候就可以使用这些标签了，只需要在 POD 的 spec 字段中添加<code>nodeSelector</code>字段，里面是我们需要被调度的节点的 label。例如，下面是我们之前的一个默认的 busybox POD 的 YAML 文件：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">labels:</span><br>    <span class="hljs-attr">app:</span> <span class="hljs-string">busybox-pod</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">test-busybox</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">containers:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">command:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">sleep</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;3600&quot;</span><br>    <span class="hljs-attr">image:</span> <span class="hljs-string">busybox</span><br>    <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">Always</span><br>    <span class="hljs-attr">name:</span> <span class="hljs-string">test-busybox</span><br><br></code></pre></td></tr></table></figure><p>然后我需要让上面的 POD 被调度到140的节点上，那么最简单的方法就是去匹配140上面的 label，如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">labels:</span><br>    <span class="hljs-attr">app:</span> <span class="hljs-string">busybox-pod</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">test-busybox</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">containers:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">command:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">sleep</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;3600&quot;</span><br>    <span class="hljs-attr">image:</span> <span class="hljs-string">busybox</span><br>    <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">Always</span><br>    <span class="hljs-attr">name:</span> <span class="hljs-string">test-busybox</span><br>  <span class="hljs-attr">nodeSelector:</span><br>    <span class="hljs-attr">source:</span> <span class="hljs-string">qikqiak</span><br><br></code></pre></td></tr></table></figure><p>然后我们可以通过 describe 命令查看调度结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kubectl describe pod test-busybox<br>......<br>Events:<br>  Type    Reason                 Age   From                    Message<br>  ----    ------                 ----  ----                    -------<br>  Normal  Scheduled              49s   default-scheduler       Successfully assigned test-busybox to 192.168.1.140<br>  Normal  SuccessfulMountVolume  49s   kubelet, 192.168.1.140  MountVolume.SetUp succeeded <span class="hljs-keyword">for</span> volume <span class="hljs-string">&quot;default-token-hmpbz&quot;</span><br>  Normal  Pulling                49s   kubelet, 192.168.1.140  pulling image <span class="hljs-string">&quot;busybox&quot;</span><br>  Normal  Pulled                 41s   kubelet, 192.168.1.140  Successfully pulled image <span class="hljs-string">&quot;busybox&quot;</span><br>  Normal  Created                41s   kubelet, 192.168.1.140  Created container<br>  Normal  Started                41s   kubelet, 192.168.1.140  Started container<br><br></code></pre></td></tr></table></figure><p>我们可以看到 Events 下面的信息，上面的 POD 被正确的调度到了140节点。通过上面的例子我们可以感受到<code>nodeSelector</code>的方式比较直观，但是还够灵活，控制粒度偏大，下面我们再看另外一种更加灵活的方式：<code>nodeAffinity</code>。</p><h2 id="nodeAffinity"><a href="#nodeAffinity" class="headerlink" title="nodeAffinity"></a>nodeAffinity</h2><p><code>nodeAffinity</code>就是节点亲和性，相对应的是<code>Anti-Affinity</code>，就是反亲和性，这种方法比上面的<code>nodeSelector</code>更加灵活，它可以进行一些简单的逻辑组合了，不只是简单的相等匹配。 调度可以分成软策略和硬策略两种方式，软策略就是如果你没有满足调度要求的节点的话，POD 就会忽略这条规则，继续完成调度过程，说白了就是<strong>满足条件最好了，没有的话也无所谓了</strong>的策略；而硬策略就比较强硬了，如果没有满足条件的节点的话，就不断重试直到满足条件为止，简单说就是<strong>你必须满足我的要求，不然我就不干</strong>的策略。 </p><p><code>nodeAffinity</code>分为以下两种策略：</p><ol><li><code>preferredDuringSchedulingIgnoredDuringExecution</code> 软策略</li><li><code>requiredDuringSchedulingIgnoredDuringExecution</code> 硬策略。</li></ol><p>如下例子：（<strong>test-node-affinity.yaml</strong>）</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">with-node-affinity</span><br>  <span class="hljs-attr">labels:</span><br>    <span class="hljs-attr">app:</span> <span class="hljs-string">node-affinity-pod</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">containers:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">with-node-affinity</span><br>    <span class="hljs-attr">image:</span> <span class="hljs-string">nginx</span><br>  <span class="hljs-attr">affinity:</span><br>    <span class="hljs-attr">nodeAffinity:</span><br>      <span class="hljs-attr">requiredDuringSchedulingIgnoredDuringExecution:</span><br>        <span class="hljs-attr">nodeSelectorTerms:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-attr">matchExpressions:</span><br>          <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">kubernetes.io/hostname</span><br>            <span class="hljs-attr">operator:</span> <span class="hljs-string">NotIn</span><br>            <span class="hljs-attr">values:</span><br>            <span class="hljs-bullet">-</span> <span class="hljs-number">192.168</span><span class="hljs-number">.1</span><span class="hljs-number">.140</span><br>            <span class="hljs-bullet">-</span> <span class="hljs-number">192.168</span><span class="hljs-number">.1</span><span class="hljs-number">.161</span><br>      <span class="hljs-attr">preferredDuringSchedulingIgnoredDuringExecution:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">weight:</span> <span class="hljs-number">1</span><br>        <span class="hljs-attr">preference:</span><br>          <span class="hljs-attr">matchExpressions:</span><br>          <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">source</span><br>            <span class="hljs-attr">operator:</span> <span class="hljs-string">In</span><br>            <span class="hljs-attr">values:</span><br>            <span class="hljs-bullet">-</span> <span class="hljs-string">qikqiak</span><br><br></code></pre></td></tr></table></figure><p>上面这个 POD 首先是要求 POD 不能运行在140和161两个节点上，如果有个节点满足<code>source=qikqiak</code>的话就优先调度到这个节点上，同样的我们可以使用<code>descirbe</code>命令查看具体的调度情况是否满足我们的要求。这里的匹配逻辑是 label 的值在某个列表中，现在<code>Kubernetes</code>提供的操作符有下面的几种：</p><ul><li>In：label 的值在某个列表中</li><li>NotIn：label 的值不在某个列表中</li><li>Gt：label 的值大于某个值</li><li>Lt：label 的值小于某个值</li><li>Exists：某个 label 存在</li><li>DoesNotExist：某个 label 不存在</li></ul><blockquote><p>如果<code>nodeSelectorTerms</code>下面有多个选项的话，满足任何一个条件就可以了；如果<code>matchExpressions</code>有多个选项的话，则必须同时满足这些条件才能正常调度 POD。</p></blockquote><h2 id="podAffinity"><a href="#podAffinity" class="headerlink" title="podAffinity"></a>podAffinity</h2><p>上面两种方式都是让 POD 去选择节点的，有的时候我们也希望能够根据 POD 之间的关系进行调度，<code>Kubernetes</code>在1.4版本引入的<code>podAffinity</code>概念就可以实现我们这个需求。</p><p>和<code>nodeAffinity</code>类似，<code>podAffinity</code>也有<code>requiredDuringSchedulingIgnoredDuringExecution</code>和 <code>preferredDuringSchedulingIgnoredDuringExecution</code>两种调度策略，唯一不同的是如果要使用互斥性，我们需要使用<code>podAntiAffinity</code>字段。 如下例子，我们希望<code>with-pod-affinity</code>和<code>busybox-pod</code>能够就近部署，而不希望和<code>node-affinity-pod</code>部署在同一个拓扑域下面：（<strong>test-pod-affinity.yaml</strong>）</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">with-pod-affinity</span><br>  <span class="hljs-attr">labels:</span><br>    <span class="hljs-attr">app:</span> <span class="hljs-string">pod-affinity-pod</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">containers:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">with-pod-affinity</span><br>    <span class="hljs-attr">image:</span> <span class="hljs-string">nginx</span><br>  <span class="hljs-attr">affinity:</span><br>    <span class="hljs-attr">podAffinity:</span><br>      <span class="hljs-attr">requiredDuringSchedulingIgnoredDuringExecution:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">labelSelector:</span><br>          <span class="hljs-attr">matchExpressions:</span><br>          <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">app</span><br>            <span class="hljs-attr">operator:</span> <span class="hljs-string">In</span><br>            <span class="hljs-attr">values:</span><br>            <span class="hljs-bullet">-</span> <span class="hljs-string">busybox-pod</span><br>        <span class="hljs-attr">topologyKey:</span> <span class="hljs-string">kubernetes.io/hostname</span><br>    <span class="hljs-attr">podAntiAffinity:</span><br>      <span class="hljs-attr">preferredDuringSchedulingIgnoredDuringExecution:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">weight:</span> <span class="hljs-number">1</span><br>        <span class="hljs-attr">podAffinityTerm:</span><br>          <span class="hljs-attr">labelSelector:</span><br>            <span class="hljs-attr">matchExpressions:</span><br>            <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">app</span><br>              <span class="hljs-attr">operator:</span> <span class="hljs-string">In</span><br>              <span class="hljs-attr">values:</span><br>              <span class="hljs-bullet">-</span> <span class="hljs-string">node-affinity-pod</span><br>          <span class="hljs-attr">topologyKey:</span> <span class="hljs-string">kubernetes.io/hostname</span><br><br></code></pre></td></tr></table></figure><p>上面这个例子中的 POD 需要调度到某个指定的主机上，至少有一个节点上运行了这样的 POD：这个 POD 有一个<code>app=busybox-pod</code>的 label。<code>podAntiAffinity</code>则是希望最好不要调度到这样的节点：这个节点上运行了某个 POD，而这个 POD 有<code>app=node-affinity-pod</code>的 label。根据前面两个 POD 的定义，我们可以预见上面这个 POD 应该会被调度到140的节点上，因为<code>busybox-pod</code>被调度到了140节点，而<code>node-affinity-pod</code>被调度到了140以为的节点，正好满足上面的需求。通过<code>describe</code>查看：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kubectl describe pod with-pod-affinity<br>......<br>Events:<br>  Type    Reason                 Age   From                    Message<br>  ----    ------                 ----  ----                    -------<br>  Normal  Scheduled              8s    default-scheduler       Successfully assigned with-pod-affinity to 192.168.1.140<br>  Normal  SuccessfulMountVolume  7s    kubelet, 192.168.1.140  MountVolume.SetUp succeeded <span class="hljs-keyword">for</span> volume <span class="hljs-string">&quot;default-token-lcl77&quot;</span><br>  Normal  Pulling                7s    kubelet, 192.168.1.140  pulling image <span class="hljs-string">&quot;nginx&quot;</span><br><br></code></pre></td></tr></table></figure><p>上面的事件信息也验证了我们的想法。</p><blockquote><p>在<code>labelSelector</code>和 <code>topologyKey</code>的同级，还可以定义 namespaces 列表，表示匹配哪些 namespace 里面的 pod，默认情况下，会匹配定义的 pod 所在的 namespace；如果定义了这个字段，但是它的值为空，则匹配所有的 namespaces。</p></blockquote><p>查看上面我们定义的3个 POD 结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kubectl get po -o wide<br>NAME                 READY     STATUS    RESTARTS   AGE       IP             NODE<br>test-busybox         1/1       Running   0          8m        172.30.95.18   192.168.1.140<br>with-node-affinity   1/1       Running   0          10m       172.30.81.25   192.168.1.172<br>with-pod-affinity    1/1       Running   0          8m        172.30.95.17   192.168.1.140<br><br></code></pre></td></tr></table></figure><p>亲和性&#x2F;反亲和性调度策略比较如下：</p><table><thead><tr><th>调度策略</th><th>匹配标签</th><th>操作符</th><th>拓扑域支持</th><th>调度目标</th></tr></thead><tbody><tr><td>nodeAffinity</td><td>主机</td><td>In, NotIn, Exists, DoesNotExist, Gt, Lt</td><td>否</td><td>指定主机</td></tr><tr><td>podAffinity</td><td>POD</td><td>In, NotIn, Exists, DoesNotExist</td><td>是</td><td>POD与指定POD同一拓扑域</td></tr><tr><td>podAnitAffinity</td><td>POD</td><td>In, NotIn, Exists, DoesNotExist</td><td>是</td><td>POD与指定POD不在同一拓扑域</td></tr></tbody></table><h2 id="污点（Taints）与容忍（tolerations）"><a href="#污点（Taints）与容忍（tolerations）" class="headerlink" title="污点（Taints）与容忍（tolerations）"></a>污点（Taints）与容忍（tolerations）</h2><p>对于<code>nodeAffinity</code>无论是硬策略还是软策略方式，都是调度 POD 到预期节点上，而<code>Taints</code>恰好与之相反，如果一个节点标记为 Taints ，除非 POD 也被标识为可以容忍污点节点，否则该 Taints 节点不会被调度pod。</p><p>比如用户希望把 Master 节点保留给 Kubernetes 系统组件使用，或者把一组具有特殊资源预留给某些 POD，则污点就很有用了，POD 不会再被调度到 taint 标记过的节点。taint 标记节点举例如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kubectl taint nodes 192.168.1.40 key=value:NoSchedule<br>node <span class="hljs-string">&quot;192.168.1.40&quot;</span> tainted<br><br></code></pre></td></tr></table></figure><p>如果仍然希望某个 POD 调度到 taint 节点上，则必须在 Spec 中做出<code>Toleration</code>定义，才能调度到该节点，举例如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">tolerations:</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">&quot;key&quot;</span><br><span class="hljs-attr">operator:</span> <span class="hljs-string">&quot;Equal&quot;</span><br><span class="hljs-attr">value:</span> <span class="hljs-string">&quot;value&quot;</span><br><span class="hljs-attr">effect:</span> <span class="hljs-string">&quot;NoSchedule&quot;</span><br><br></code></pre></td></tr></table></figure><p>effect 共有三个可选项，可按实际需求进行设置：</p><ol><li><code>NoSchedule</code>：除非具有匹配的容忍度规约，否则新的 Pod 不会被调度到带有污点的节点上。 当前正在节点上运行的 Pod 不会被驱逐。</li><li><code>PreferNoSchedule</code>：PreferNoSchedule 是“偏好”或“软性”的 NoSchedule。 控制平面将尝试避免将不能容忍污点的 Pod 调度到的节点上，但不能保证完全避免。</li><li><code>NoExecute</code>：该选项意味着一旦 Taint 生效，如该节点内正在运行的 POD 没有对应 Tolerate 设置，会直接被逐出。</li></ol><h2 id="Refer"><a href="#Refer" class="headerlink" title="Refer"></a>Refer</h2><ul><li><a href="https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/">https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/</a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/">https://kubernetes.io/docs/concepts/configuration/assign-pod-node/</a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/">https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/</a></li><li><a href="https://coreos.com/fleet/docs/latest/affinity.html">https://coreos.com/fleet/docs/latest/affinity.html</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes Probes Intros</title>
    <link href="/2024/07/31/Kubernetes-Probes/"/>
    <url>/2024/07/31/Kubernetes-Probes/</url>
    
    <content type="html"><![CDATA[<h2 id="Probes-Introduction"><a href="#Probes-Introduction" class="headerlink" title="Probes Introduction"></a>Probes Introduction</h2><ul><li><p><strong>存活性探测 (Liveness probes):</strong> 存活性探针，用于判断容器是不是健康，如果不满足健康条件，那么 Kubelet 将根据 Pod 中设置的 restartPolicy （重启策略）来判断，Pod 是否要进行重启操作。</p><blockquote><p>LivenessProbe 按照配置去探测 ( 进程、或者端口、或者命令执行后是否成功等等)，来判断容器是不是正常。如果探测不到，代表容器不健康（可以配置连续多少次失败才记为不健康），则 kubelet 会杀掉该容器，并根据容器的重启策略做相应的处理。如果未配置存活探针，则默认容器启动为通过（Success）状态。即探针返回的值永远是 Success。即 Success 后 pod 状态是 RUNING</p></blockquote></li><li><p><strong>就绪性探测 (Readiness probes):</strong> 就绪性探针，用于判断容器内的程序是否存活（或者说是否健康），只有程序(服务)正常， 容器开始对外提供网络访问（启动完成并就绪）。</p><blockquote><p>容器启动后按照 ReadinessProbe 配置进行探测，无问题后结果为成功即状态为 Success。pod 的 READY 状态为 true，从 0&#x2F;1 变为 1&#x2F;1。如果失败继续为 0&#x2F;1，状态为 false。若未配置就绪探针，则默认状态容器启动后为 Success。对于此 pod、此 pod 关联的 Service 资源、EndPoint 的关系也将基于 Pod 的 Ready 状态进行设置，如果 Pod 运行过程中 Ready 状态变为 false，则系统自动从 Service 资源 关联的 EndPoint 列表中去除此 pod，届时 service 资源接收到 GET 请求后，kube-proxy 将一定不会把流量引入此 pod 中，通过这种机制就能防止将流量转发到不可用的 Pod 上。如果 Pod 恢复为 Ready 状态。将再会被加回 Endpoint 列表。kube-proxy 也将有概率通过负载机制会引入流量到此 pod 中。</p></blockquote></li><li><p><strong>启动探测 (Startup Probes):</strong> 对于旧应用需要更长的启动时间，这时候既不想重启应用也不想让请求访问进来，可以设置启动探测给足够的启动时间保证应用启动成功</p></li></ul><h2 id="Execution-Order"><a href="#Execution-Order" class="headerlink" title="Execution Order"></a>Execution Order</h2><p>可以自定义在 pod 启动时是否执行这些检测，如果不设置，则检测结果均默认为通过；</p><p>如果设置，则顺序为： <code>startupProbe --&gt; readinessProbe --&gt; livenessProbe</code></p><p>但是真的是上面的顺序嘛？ 官方文档上却说不是，其中有一段说到。</p><blockquote><p>Caution: Liveness probes do not wait for readiness probes to succeed. If you want to wait before executing a liveness probe you should use initialDelaySeconds or a startupProbe.</p></blockquote><blockquote><p>livenessProbe 不会等待 readinessProbe 成功。如果你想在执行 livenessProbe 之前等待，你应该使用 initialDelaySeconds 或 startupProbe。</p></blockquote><p>所以真正的顺序是：<code>startupProbe --&gt; readinessProbe(livenessProbe)</code></p><p>readinessProbe 和 livenessProbe 之间是异步并发的。</p><h2 id="LivenessProbe-ReadinessProbe-Usage"><a href="#LivenessProbe-ReadinessProbe-Usage" class="headerlink" title="LivenessProbe &amp; ReadinessProbe Usage"></a>LivenessProbe &amp; ReadinessProbe Usage</h2><h3 id="探测方法"><a href="#探测方法" class="headerlink" title="探测方法"></a>探测方法</h3><p>LivenessProbe 和 ReadinessProbe 两种探针都支持下面三种探测方法：</p><ul><li>ExecAction：在容器中执行指定的命令，如果执行成功，退出码为 0 则探测成功。</li><li>HTTPGetAction：通过容器的 IP 地址、端口号及路径调用 HTTP Get 方法，如果响应的状态码大于等于 - 200 且小于 400，则认为容器 健康。</li><li>TCPSocketAction：通过容器的 IP 地址和端口号执行 TCP 检 查，如果能够建立 TCP 连接，则表明容器健康。</li></ul><h2 id="探测参数"><a href="#探测参数" class="headerlink" title="探测参数"></a>探测参数</h2><p>LivenessProbe 和 ReadinessProbe 两种探针的相关属性<br>探针(Probe)有许多可选字段，可以用来更加精确的控制 Liveness 和 Readiness 两种探针的行为(Probe)：</p><ul><li><p>initialDelaySeconds：容器启动后要等待多少秒后就探针开始工作，单位“秒”，默认是 0 秒，最小值是 0</p></li><li><p>periodSeconds：执行探测的时间间隔（单位是秒），默认为 10s，单位“秒”，最小值是 1</p></li><li><p>timeoutSeconds：探针执行检测请求后，等待响应的超时时间，默认为 1s，单位“秒”，最小值是 1</p></li><li><p>successThreshold：探针检测失败后认为成功的最小连接成功次数，默认为 1s，在 Liveness 探针中必须为 1s，最小值为 1s。</p></li><li><p>failureThreshold：探测失败的重试次数，重试一定次数后将认为失败，在 readiness 探针中，Pod 会被标记为未就绪，默认为 3s，最小值为 1s</p><blockquote><p>Tips：initialDelaySeconds 在 ReadinessProbe 其实可以不用配置，不配置默认 pod 刚启动，开始进行 ReadinessProbe 探测，等服务启动后并检查 success 成功后，READY 状态自然正常。</p></blockquote></li></ul><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span><br><span class="hljs-attr">metadata:</span><br>    <span class="hljs-attr">name:</span> <span class="hljs-string">liveness-http</span><br>    <span class="hljs-attr">labels:</span><br>        <span class="hljs-attr">test:</span> <span class="hljs-string">liveness</span><br><span class="hljs-attr">spec:</span><br>    <span class="hljs-attr">containers:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">liveness</span><br>          <span class="hljs-attr">image:</span> <span class="hljs-string">test.com/test-http-prober:v0.0.1</span><br>          <span class="hljs-attr">LivenessProbe:</span><br>              <span class="hljs-attr">failureThreshold:</span> <span class="hljs-number">5</span> <span class="hljs-comment">#检测失败5次表示未就绪</span><br>              <span class="hljs-attr">initialDelaySeconds:</span> <span class="hljs-number">20</span> <span class="hljs-comment">#延迟加载时间</span><br>              <span class="hljs-attr">periodSeconds:</span> <span class="hljs-number">10</span> <span class="hljs-comment">#重试时间间隔</span><br>              <span class="hljs-attr">timeoutSeconds:</span> <span class="hljs-number">5</span> <span class="hljs-comment">#超时时间设置</span><br>              <span class="hljs-attr">successThreshold:</span> <span class="hljs-number">2</span> <span class="hljs-comment">#检查成功为2次表示就绪</span><br>              <span class="hljs-attr">httpGet:</span><br>                  <span class="hljs-attr">port:</span> <span class="hljs-number">8082</span><br>                  <span class="hljs-attr">path:</span> <span class="hljs-string">/status/health</span><br></code></pre></td></tr></table></figure><h2 id="Refer"><a href="#Refer" class="headerlink" title="Refer"></a>Refer</h2><ul><li><a href="https://juejin.cn/post/7163135179177852936">https://juejin.cn/post/7163135179177852936</a></li><li><a href="https://juejin.cn/post/7163135453489528845">https://juejin.cn/post/7163135453489528845</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Systemd Journal Logs Too Big Debug</title>
    <link href="/2024/06/26/systemd-log-too-big/"/>
    <url>/2024/06/26/systemd-log-too-big/</url>
    
    <content type="html"><![CDATA[<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>最近的机器磁盘容量经常报警,发现以下目录的磁盘占用较多</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo <span class="hljs-built_in">du</span> -sh ./* --exclude ./proc | <span class="hljs-built_in">sort</span> -rh  | <span class="hljs-built_in">head</span> -n 5<br></code></pre></td></tr></table></figure><p><strong><code>/run/log/journal</code></strong> 是 Systemd 的日志目录，用于存储 Systemd 日志文件。你可以使用 <strong><code>journalctl</code></strong> 工具来管理和清理这些日志文件。以下是清理 Systemd 日志的方法.</p><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><h3 id="1-列出日志条目："><a href="#1-列出日志条目：" class="headerlink" title="1. 列出日志条目："></a>1. 列出日志条目：</h3><p>若要查看可用的日志条目，可以使用以下命令：</p><pre><code class="hljs"><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">journalctl</span><br></code></pre></td></tr></table></figure>这将显示当前可用的 Systemd 日志。</code></pre><h3 id="2-设置日志保留策略："><a href="#2-设置日志保留策略：" class="headerlink" title="2. 设置日志保留策略："></a>2. 设置日志保留策略：</h3><p>Systemd 使用日志保留策略来管理日志文件的保留时间。你可以通过编辑 <strong><code>/etc/systemd/journald.conf</code></strong> 文件来配置日志保留策略。查找 <strong><code>SystemKeepFree</code><strong>、</strong><code>SystemMaxUse</code></strong> 和 <strong><code>RuntimeMaxUse</code></strong> 选项，然后根据需要进行设置。例如：</p><pre><code class="hljs"><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs makefile"><br>SystemMaxUse=100M<br></code></pre></td></tr></table></figure>这将配置 Systemd 日志最大占用的磁盘空间为 100MB。你可以根据你的需求调整此值。</code></pre><h3 id="3-清理日志："><a href="#3-清理日志：" class="headerlink" title="3. 清理日志："></a>3. 清理日志：</h3><p>一旦你设置了日志保留策略，你可以运行以下命令来清理日志：</p><pre><code class="hljs"><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css">sudo journalctl <span class="hljs-attr">--vacuum-time</span>=<span class="hljs-number">1</span>h<br></code></pre></td></tr></table></figure>这将删除三天前的日志，保留最近三天的日志。你可以更改 **`--vacuum-time`** 标志的值来定义保留多长时间的日志。</code></pre><h3 id="4-查看清理后的日志大小："><a href="#4-查看清理后的日志大小：" class="headerlink" title="4. 查看清理后的日志大小："></a>4. 查看清理后的日志大小：</h3><p>若要验证清理后的日志文件大小，可以再次运行以下命令：</p><pre><code class="hljs"><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">journalctl</span><br></code></pre></td></tr></table></figure>这将显示当前可用的日志信息，包括已清理的部分。</code></pre><h2 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h2><p>请注意，清理 Systemd 日志需要足够的权限。确保你有足够的权限来执行这些操作，或者使用 <strong><code>sudo</code></strong> 命令以管理员权限执行。清理日志文件可能会释放磁盘空间，但也可能会导致丢失一些旧日志信息，因此请根据你的需求和存储空间情况慎重选择清理策略。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Debug</tag>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes FailedScheduling Debug</title>
    <link href="/2024/06/25/Kubernetes-FailedScheduling-Debug/"/>
    <url>/2024/06/25/Kubernetes-FailedScheduling-Debug/</url>
    
    <content type="html"><![CDATA[<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>正在使用Kubernetes进行Druid部署,发现更改sts的replica之后pod一直处于pending状态,接下来通过describe查看pod的部署状态,发现以下log</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">nodeSelector_new:</span><br>  <span class="hljs-attr">dedicated:</span> <span class="hljs-string">druid-middlemanager</span><br><br><span class="hljs-attr">tolerations:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">&quot;dedicated&quot;</span><br>    <span class="hljs-attr">operator:</span> <span class="hljs-string">&quot;Equal&quot;</span><br>    <span class="hljs-attr">value:</span> <span class="hljs-string">&quot;druid&quot;</span><br>    <span class="hljs-attr">effect:</span> <span class="hljs-string">&quot;NoSchedule&quot;</span><br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">Node-Selectors:              dedicated=druid-middlemanager<br>Tolerations:                 dedicated=druid:NoSchedule<br><br>Events:<br>  Type     Reason             Age                 From                Message<br>  ----     ------             ----                ----                -------<br>  Normal   NotTriggerScaleUp  117s                cluster-autoscaler  pod didn<span class="hljs-string">&#x27;t trigger scale-up: 1 node(s) had untolerated taint &#123;dedicated: ***&#125; .......</span><br><span class="hljs-string">  Warning  FailedScheduling   10s (x9 over 2m8s)  default-scheduler   0/84 nodes are available: 1 Insufficient memory, 1 node(s) had untolerated taint &#123;dedicated: ****&#125; ...</span><br></code></pre></td></tr></table></figure><h2 id="Debug"><a href="#Debug" class="headerlink" title="Debug"></a>Debug</h2><p>第一反应认为可能是以下Bugs</p><ol><li>nodegroup资源的label错误</li><li>sts资源的label错误</li><li>nodegroup的auto scale失效</li></ol><p>通过检查sts 和 nodegroup的dedicated和tolerations发现并没有问题,排除了Bug 1和Bug 2,重点放在Bug 3。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">K8s Labels: foc/bottlerocket=<span class="hljs-literal">true</span> dedicated=druid-middlemanager<br>K8s Taints: dedicated=druid:NoSchedule<br></code></pre></td></tr></table></figure><p>目前nodegroup内已经有了一个node,如果资源不够的话会在node group的activities有尝试扩容的log,但是并没有,因此并不是扩容能解决的问题。<br>因此排除了Bug 3。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>最终发现是nodegroup的机型内存较小,即便扩容也无法提供这么大内存的node供单个pod使用,因此会导致以上报错。<br>这个logs很容易让人往资源label不匹配的方面或者auto scale失效的方面去想,很隐蔽的bug。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">resources:</span><br>  <span class="hljs-attr">limits:</span><br>    <span class="hljs-attr">memory:</span> <span class="hljs-string">110Gi</span><br>  <span class="hljs-attr">requests:</span><br>    <span class="hljs-attr">memory:</span> <span class="hljs-string">110Gi</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>Debug</tag>
      
      <tag>Druid</tag>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HDFS Decomission Codes and Intros</title>
    <link href="/2024/06/17/HDFS-Decomission-Codes-and-Introduction/"/>
    <url>/2024/06/17/HDFS-Decomission-Codes-and-Introduction/</url>
    
    <content type="html"><![CDATA[<h1 id="HDFS-Decomission-Codes-and-Intros"><a href="#HDFS-Decomission-Codes-and-Intros" class="headerlink" title="HDFS Decomission Codes and Intros"></a>HDFS Decomission Codes and Intros</h1><h2 id="Background-of-HDFS-decomission"><a href="#Background-of-HDFS-decomission" class="headerlink" title="Background of HDFS decomission"></a>Background of HDFS decomission</h2><p> Currently there are 200+ 1.22PB HDFS3 and 100- 480T HDFS nodes, the decommissioning nodes are slow and can only migrate about 150k blocks per hour, on average a node has about 600k blocks and one takes about 4 hours.</p><h2 id="Difference-between-node-drop-and-node-retirement"><a href="#Difference-between-node-drop-and-node-retirement" class="headerlink" title="Difference between node drop and node retirement?"></a><strong>Difference between node drop and node retirement?</strong></h2><h3 id="Node-Decommissioning"><a href="#Node-Decommissioning" class="headerlink" title="Node Decommissioning"></a><strong>Node Decommissioning</strong></h3><p> First of all, normally add the node to the decommissioning list, first tell namenode and yarn not to submit new tasks and write data up; and then wait for the node on the data block in the cluster replication is complete; <strong>this time the decommissioned node is prioritized as the srcNode data source (preferred to choose the decommissioned node as a replicated data source src, because it has no write requests, low load</strong> ), the other nodes from the <strong>decommissioned</strong> node to <strong>replicate data source src</strong>. Other nodes copy data blocks from this retired node to other nodes, so the load on this node will be high at this time. All data blocks are replicated and the node status becomes Decommissioned, which can be viewed from the namenodeURL interface. <strong>Note that the data node decommissioned data to start replication time is also 10 minutes and 30s later, and will not start earlier because it is actively decommissioned, because the namenode and datanode will always maintain a simple master-slave relationship, the namenode node will not actively initiate any IPC calls to the datanode node, the datanode node needs to cooperate with the All operations completed by the namenode are returned through the DatanodeCommand carried by both heartbeat answers.</strong></p><h3 id="Node-Drop"><a href="#Node-Drop" class="headerlink" title="Node Drop"></a>Node Drop</h3><p> For example, forced stop datanode, the physical machine hangs (such as high load drop, sudden network failure, hardware failure, etc.), these are node drop, the general default 10 minutes after 30s (mainly controlled by two parameters) namenode will detect the node communication anomaly drop. Then namenode according to the node’s ip, find out all the node’s blockid, as well as the corresponding copy of the machine, through the heartbeat mechanism to arrange for data replication, this time the data replication, the data source is not in the node is not down, but one of the multiple copies of the node, the same this time the copy replication also follow the rack-awareness, copy shelving strategy.</p><aside>💡  The difference between node drop and decommissioning of the two is not only the data replication method is different, there is also namenode Under-Replicated Blocks of data replication strategy is not the same (data block block replication level is divided into five kinds); extreme examples such as single-copy node decommissioning data will not be lost, a single-copy node drop data will be really lost;</aside><h3 id="Network-storm-caused-by-node-drop"><a href="#Network-storm-caused-by-node-drop" class="headerlink" title="Network storm caused by node drop"></a>Network storm caused by node drop</h3><p> dozens of T, or even hundreds of T, millions of block node drop, there will be a large number of RPC storms, especially for large-scale high-load clusters on namenode is a big challenge, not only affects the performance of the production, but also there will be a great deal of hidden danger, especially for the bandwidth bottleneck of the limitations of clusters.</p><p> Generally speaking the value of namenode to detect whether datanode is dropped is 10<em>3s (heartbeat time) + 2</em>5min (namenode detection time, the parameter is: <code>dfs.namenode.heartbeat.recheck-interval</code> ) &#x3D; 10min30s. if within 10min30s of time bandwidth continues to hit full, RPC requests are delayed, and datanode and namenode nodes are not communicating well, it is easy to cause other nodes to continue to fall offline, forming a vicious cycle, how should this situation be avoided?</p><p> NameNode maintains a replication priority queue, for the file block with insufficient replicas, the file block with only one replica left will enjoy the highest replication priority. So if you look at a cluster with two replicas, as long as there is an exception in one block, there will be only one replica left, which is the highest-priority block replica, and it will be storm mode replication, which will easily affect the cluster performance if not well controlled, and even hang the cluster. Therefore, it is generally not recommended that the cluster copy factor is 2.</p><aside>💡  L1 (highest): there is a risk of data loss of the block, such as: 1. Only one copy of the block (especially for 2 copies of the block, down a node) or these blocks have 0 active copy; 2, single copy in the node is being decommissioned to own the block.<p> L2:Block replicas whose actual value is much lower than the configured value (e.g., 3 replicas, 2 missing), i.e., blocks whose replica count is less than 1&#x2F;3 of the expected value, and these blocks are replicated with second priority. For example, 4 copies of block, there are 3 lost or broken, it will be replicated with priority than 4 copies of block missing 2.</p><p> L3:Insufficient copies are not as high as those with priority L2, which are replicated first. Third priority.</p><p> L4:block meets the minimum number of copies required. The replica degree requirement is lower than both L2-L3.</p><p> L5:Damaged blocks and currently have available non-damaged replicas</p></aside><h3 id="Parameters-control-RPC-storms-caused-by-node-drops"><a href="#Parameters-control-RPC-storms-caused-by-node-drops" class="headerlink" title="Parameters control RPC storms caused by node drops"></a>Parameters control RPC storms caused by node drops</h3><p> The three parameters are <code>hdfs-site.xml</code> parameters, you can refer to the apache hadoop official website, in fact, there are two aspects of the block replication speed to determine, one is the speed of namenode distribution tasks, the second is the speed of replication between datanode. The former can be understood as the entrance and the latter can be treated as the exit.</p><blockquote><p>These parameters are not available in hadoop 2.7.3</p></blockquote><h4 id="Ingress-Parameters"><a href="#Ingress-Parameters" class="headerlink" title="Ingress Parameters"></a>Ingress Parameters</h4><p> Controls the distribution of tasks from the namenode level. Changes to this parameter require <strong>a restart of the namenode</strong>, not the datanode.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">dfs.namenode.replication.work.multiplier.per.iteration <br>default=2<br></code></pre></td></tr></table></figure><p> This parameter determines the number of blocks that each DN is told to replicate when the NN has a heartbeat (3s) with the DN to send the task list. For example, if the cluster has 500 nodes and this value is set to 10, then the number of data blocks that a heartbeat namnode can send datanode to replicate is 10*500&#x3D;5000 blocks.</p><p> If a node drops&#x2F;retires and there are 800000 blocks to be replicated, how long does it take for the namenode to finish distributing the task of replicating the blocks to the datanode.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-string">task</span> <span class="hljs-string">distribution</span> <span class="hljs-string">time</span> <span class="hljs-string">=</span> <br><span class="hljs-string">wait</span> <span class="hljs-string">for</span> <span class="hljs-string">copy</span> <span class="hljs-string">block</span> <span class="hljs-string">number</span> <span class="hljs-string">/</span> <span class="hljs-string">(datanode</span> <span class="hljs-string">number</span> <span class="hljs-string">*</span> <span class="hljs-string">parameters)</span> <span class="hljs-string">*</span> <span class="hljs-string">heartbeat</span> <span class="hljs-string">interval</span> <span class="hljs-string">time</span><br></code></pre></td></tr></table></figure><h4 id="Export-Parameters"><a href="#Export-Parameters" class="headerlink" title="Export Parameters"></a>Export Parameters</h4><p> In contrast to the above, which controls task distribution from the nanode, the following two parameters are controlled at the datanode level, and require <strong>a restart of the namenode</strong>.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-string">dfs.namenode.replication.max-streams</span><br><span class="hljs-string">default=2</span><br></code></pre></td></tr></table></figure><p> The meaning of this parameter is to control the datanode node to carry out data replication of the maximum number of threads, from the above we know that the block replication priority is divided into five kinds. This parameter controls the replication of blocks that do not contain the highest priority. This parameter controls the replication of blocks that do not contain the highest priority.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-string">dfs.namenode.replication.max-streams-hard-limit</span><br><span class="hljs-string">hadoop</span> <span class="hljs-string">default=2,</span> <span class="hljs-string">cdh</span> <span class="hljs-string">default=40</span><br></code></pre></td></tr></table></figure><p> The meaning of this parameter is to control the number of streams copied by all priority blocks of the datanode, including the highest priority; generally used in conjunction with the above and above two parameters each other.</p><aside>💡  The former parameter controls how often the datanode accepts tasks, and the latter two parameters further limit the maximum amount of parallel threaded network transfers that the DataNode can accomplish at one time. How much to set the value of the above parameters depends on the cluster size and cluster configuration, and cannot be determined in the same way. Generally speaking, it is simpler and easier to control from the entrance. For example, the scale of 500 clusters, dfs.namenode.replication.work.multiplier.per.iteration = 10, then the cluster heartbeat distribution of 5000 blocks at a time, if the cluster file storage is broken up in all 500 nodes, each node at the same time replicated 10 blocks (). The actual will be because of the replica shelving policy, rack awareness and so on will not be all the nodes are involved in data replication), each block size 128Mb, then each node's network load is 128 * 10/3 = 546Mb / s, then you have to look at the combination of the actual bandwidth bottlenecks, such a large network IO will affect the normal task of the computation, if so, the value of this If there is, the value should be adjusted down a bit.</aside><h2 id="How-to-go-offline-quickly"><a href="#How-to-go-offline-quickly" class="headerlink" title="How to go offline quickly"></a>How to go offline quickly</h2><p> The essence of how to make nodes go offline quickly is to improve the replication speed of the replica. It is mainly controlled by the above three parameters. The first is to control namenode task distribution, and the second control datanode replication rate, provided that it does not affect the normal production tasks. The smaller the cluster size, the slower the downline, for example, because the total number of distribution will be much slower.</p><p> For example, a 500-unit node <code>dfs.namenode.replication.work.multiplier.per.iteration=10</code>, a 50-unit node this value should be set to 100, both namenode task distribution speed can be consistent. Specifically combined with the actual cluster size settings.</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs json">&lt;property&gt;<br>    &lt;name&gt;dfs.namenode.replication.max-streams&lt;/name&gt;<br>    &lt;value&gt;<span class="hljs-number">10</span>&lt;/value&gt;<br>&lt;/property&gt;<br>&lt;property&gt;<br>    &lt;name&gt;dfs.namenode.replication.max-streams-hard-limit&lt;/name&gt;<br>    &lt;value&gt;<span class="hljs-number">20</span>&lt;/value&gt;<br>&lt;/property&gt;<br>&lt;property&gt;<br>    &lt;name&gt;dfs.namenode.replication.work.multiplier.per.iteration&lt;/name&gt;<br>    &lt;value&gt;<span class="hljs-number">5</span>&lt;/value&gt;<br>&lt;/property&gt;<br></code></pre></td></tr></table></figure><p> In the test cluster, 15 HDFS, an average of 80k blocks per unit, can migrate 63k blocks per hour, than the initial 12k accelerated by 5 times.</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs json"># cdh 配置<br>dfs.namenode.replication.work.multiplier.per.iteration=<span class="hljs-number">4</span><br>dfs.namenode.replication.max-streams=<span class="hljs-number">20</span>   <br>dfs.namenode.replication.max-streams-hard-limit=<span class="hljs-number">40</span><br></code></pre></td></tr></table></figure><h4 id="Impact"><a href="#Impact" class="headerlink" title="Impact"></a>Impact</h4><p> Significantly increase CPU Load, CPU IO Wait, Disk IO Wait and other resource utilization.</p><h3 id="Source-Code-Analysis"><a href="#Source-Code-Analysis" class="headerlink" title="Source Code Analysis"></a>Source Code Analysis</h3><p> Once the address of <code>dfs.hosts.exclude</code> is written to conf, refreshNodes can get the exclude list and operate on it.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * Rereads conf to get hosts and exclude list file names.</span><br><span class="hljs-comment"> * Rereads the files to update the hosts and exclude lists.  It</span><br><span class="hljs-comment"> * checks if any of the hosts have changed states:</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">refreshNodes</span><span class="hljs-params">(<span class="hljs-keyword">final</span> Configuration conf)</span> <span class="hljs-keyword">throws</span> IOException &#123;<br>  refreshHostsReader(conf);<br>  namesystem.writeLock();<br>  <span class="hljs-keyword">try</span> &#123;<br>    refreshDatanodes();<br>    countSoftwareVersions();<br>  &#125; <span class="hljs-keyword">finally</span> &#123;<br>    namesystem.writeUnlock();<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p> Read the address of the node through <code>dfs.exclude</code>, then mark it as decommissioned and use monitor to tracking Node.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs bash">public void startDecommission(DatanodeDescriptor node) &#123;<br>  <span class="hljs-keyword">if</span> (!node.isDecommissionInProgress() &amp;&amp; !node.isDecommissioned()) &#123;<br>    // Update DN stats maintained by HeartbeatManager<br>    hbManager.startDecommission(node);<br>    // hbManager.startDecommission will <span class="hljs-built_in">set</span> dead node to decommissioned.<br>    <span class="hljs-keyword">if</span> (node.isDecommissionInProgress()) &#123;<br>      <span class="hljs-keyword">for</span> (DatanodeStorageInfo storage : node.getStorageInfos()) &#123;<br>        LOG.info(<span class="hljs-string">&quot;Starting decommission of &#123;&#125; &#123;&#125; with &#123;&#125; blocks&quot;</span>,<br>            node, storage, storage.numBlocks());<br>      &#125;<br>      node.getLeavingServiceStatus().setStartTime(monotonicNow());<br>      monitor.startTrackingNode(node);<br>    &#125;<br>  &#125; <span class="hljs-keyword">else</span> &#123;<br>    LOG.trace(<span class="hljs-string">&quot;startDecommission: Node &#123;&#125; in &#123;&#125;, nothing to do.&quot;</span>,<br>        node, node.getAdminState());<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p> Before decommissioning, the cluster stats will be pre-decimated, such as <code>capacityUsed</code>, <code>capacityTotal</code>, and so on.</p><p> monitor’s run() will call processPendingNodes() to take out nodes from the <code>pending nodes</code> and decommission them.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * Move any pending nodes into outOfServiceNodeBlocks to initiate the</span><br><span class="hljs-comment"> * decommission or maintenance mode process.</span><br><span class="hljs-comment"> *</span><br><span class="hljs-comment"> * This method must be executed under the namenode write lock to prevent</span><br><span class="hljs-comment"> * the pendingNodes list from being modified externally.</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">private</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">processPendingNodes</span><span class="hljs-params">()</span> &#123;<br>  <span class="hljs-keyword">while</span> (!pendingNodes.isEmpty() &amp;&amp;<br>      (maxConcurrentTrackedNodes == <span class="hljs-number">0</span> ||<br>          outOfServiceNodeBlocks.size() &lt; maxConcurrentTrackedNodes)) &#123;<br>    outOfServiceNodeBlocks.put(pendingNodes.poll(), <span class="hljs-literal">null</span>);<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p> After getting the nodes, it will call the check() method, which will get the block information of these nodes and process the PendingReplication Blocks.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">private</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">check</span><span class="hljs-params">()</span> &#123;<br>  <span class="hljs-keyword">final</span> List&lt;DatanodeDescriptor&gt; toRemove = <span class="hljs-keyword">new</span> <span class="hljs-title class_">ArrayList</span>&lt;&gt;();<br><br>  <span class="hljs-keyword">if</span> (outOfServiceNodeBlocks.size() == <span class="hljs-number">0</span>) &#123;<br>    <span class="hljs-comment">// No nodes currently being tracked so simply return</span><br>    <span class="hljs-keyword">return</span>;<br>  &#125;<br><br>  <span class="hljs-comment">// Check if there are any pending nodes to process, ie those where the</span><br>  <span class="hljs-comment">// storage has not been scanned yet. For all which are pending, scan</span><br>  <span class="hljs-comment">// the storage and load the under-replicated block list into</span><br>  <span class="hljs-comment">// outOfServiceNodeBlocks. As this does not modify any external structures</span><br>  <span class="hljs-comment">// it can be done under the namenode *read* lock, and the lock can be</span><br>  <span class="hljs-comment">// dropped between each storage on each node.</span><br>  <span class="hljs-comment">//</span><br>  <span class="hljs-comment">// TODO - This is an expensive call, depending on how many nodes are</span><br>  <span class="hljs-comment">//        to be processed, but it requires only the read lock and it will</span><br>  <span class="hljs-comment">//        be dropped and re-taken frequently. We may want to throttle this</span><br>  <span class="hljs-comment">//        to process only a few nodes per iteration.</span><br>  outOfServiceNodeBlocks.keySet()<br>      .stream()<br>      .filter(n -&gt; outOfServiceNodeBlocks.get(n) == <span class="hljs-literal">null</span>)<br>      .forEach(n -&gt; scanDatanodeStorage(n, <span class="hljs-literal">true</span>));<br><br>  processMaintenanceNodes();<br>  <span class="hljs-comment">// First check the pending replication list and remove any blocks</span><br>  <span class="hljs-comment">// which are now replicated OK. This list is constrained in size so this</span><br>  <span class="hljs-comment">// call should not be overly expensive.</span><br>  processPendingReplication();<br><br>  <span class="hljs-comment">// Now move a limited number of blocks to pending</span><br>  moveBlocksToPending();<br><br>  <span class="hljs-comment">// Check if any nodes have reached zero blocks and also update the stats</span><br>  <span class="hljs-comment">// exposed via JMX for all nodes still being processed.</span><br>  checkForCompletedNodes(toRemove);<br><br>  <span class="hljs-comment">// Finally move the nodes to their final state if they are ready.</span><br>  processCompletedNodes(toRemove);<br>&#125;<br></code></pre></td></tr></table></figure><ol><li>processPendingReplication() method will call isBlockReplicatedOk() method (the core) will determine whether the block needs reconstruction, and put the information into the <code>*replication queue*</code> in the <code>blockManager</code>.</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs java">blockManager.neededReconstruction.add(block,<br>    liveReplicas, num.readOnlyReplicas(),<br>    num.outOfServiceReplicas(),<br>    blockManager.getExpectedRedundancyNum(block));<br></code></pre></td></tr></table></figure><ol start="2"><li>The moveBlocksToPending() method determines the length of the queue and the limit of the pending replication, and then creates a block iterator for each node that needs maintenance or delegation, and then each block will continue to call the above mentioned isBlockReplicatedOk() method until the block replication limit is reached, then stop and exit the loop.</li></ol><p> The moveBlocksToPending() method has an interesting piece of locking code.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs java">Iterator&lt;DatanodeDescriptor&gt; nodeIter =<br>    Iterables.cycle(iterators.keySet()).iterator();<br><span class="hljs-keyword">while</span> (nodeIter.hasNext()) &#123;<br>  <span class="hljs-comment">// Cycle through each node with blocks which still need processed</span><br>  <span class="hljs-type">DatanodeDescriptor</span> <span class="hljs-variable">dn</span> <span class="hljs-operator">=</span> nodeIter.next();<br>  Iterator&lt;BlockInfo&gt; blockIt = iterators.get(dn);<br>  <span class="hljs-keyword">while</span> (blockIt.hasNext()) &#123;<br>    <span class="hljs-comment">// Process the blocks for the node until we find one that needs</span><br>    <span class="hljs-comment">// replication</span><br>    <span class="hljs-keyword">if</span> (blocksProcessed &gt;= blocksPerLock) &#123;<br>      blocksProcessed = <span class="hljs-number">0</span>;<br>      namesystem.writeUnlock();<br>      namesystem.writeLock();<br>    &#125;<br>    blocksProcessed++;<br>    <span class="hljs-keyword">if</span> (nextBlockAddedToPending(blockIt, dn)) &#123;<br>      <span class="hljs-comment">// Exit the inner &quot;block&quot; loop so an iterator for the next datanode</span><br>      <span class="hljs-comment">// is used for the next block.</span><br>      pendingCount++;<br>      <span class="hljs-keyword">break</span>;<br>    &#125;<br>  &#125;<br></code></pre></td></tr></table></figure><p> It releases the write lock and then immediately requests a write lock because the <strong><code>blocksProcessed</code></strong> variable is used to keep track of the number of blocks that have been processed. When the number of blocks processed reaches a predetermined threshold of <strong><code>blocksPerLock</code></strong>, it gives other threads waiting for a write lock a chance to execute by releasing the current write lock and acquiring it again. This helps avoid blocking other threads by holding the lock for a long period of time, thus improving concurrency performance.</p><aside>💡<p> Why does the check function call processPendingReplication and then moveBlocksToPending? Can’t it just call one?</p><p> In some cases it is possible to call only one function to process the replication state of a block, but in some cases it may be necessary to call both functions. Let’s see why it is necessary to call both functions in some cases:</p><ol><li><strong><code>processPendingReplication</code></strong>:<ul><li>The main task of this function is to process the blocks that are currently in the “pending replication” state and make sure that they are replicated to the required number of copies. It is usually called periodically by the scheduler to make sure that the blocks in the cluster maintain sufficient redundancy.</li><li>When this function is called, it checks to see which blocks are currently in the pending state and attempts to start replication tasks to replicate those blocks.</li></ul></li><li><strong><code>moveBlocksToPending</code></strong>:<ul><li>The main task of this function is to move blocks to the “pending replication” queue under certain circumstances, e.g., when a node enters maintenance mode or goes offline, to wait for a subsequent replication task.</li><li>When this function is called, it checks if any node is currently in maintenance mode or offline, and then adds the relevant block to the pending replication queue.</li></ul></li></ol><p> Why do we need to call both functions at the same time? Because they don’t deal with exactly the same issues:</p><ul><li><strong><code>processPendingReplication</code></strong> focuses on blocks that are currently pending, while <strong><code>moveBlocksToPending</code></strong> focuses on checking the blocks that need to be processed after a node’s state has changed.</li><li>In some cases, new blocks may need to be replicated, or some blocks may need to be moved to the queue waiting to be replicated due to a node state change, so calling both functions simultaneously ensures that the processing of block replication status is comprehensive and timely.</li></ul></aside><ol><li>The checkForCompletedNodes method iterates through the state of the nodes and adds them to the toRemove list if the migration of the blocks is complete.</li><li>The processCompletedNodes method gets the toRemove list, traverses it and sets the status, for example to decommissioned, and removes it from <strong><code>outOfServiceNodeBlocks</code></strong> and <strong><code>pendingRep.</code></strong></li></ol><hr><h1 id="HDFS-Decomission-代码解读与分析"><a href="#HDFS-Decomission-代码解读与分析" class="headerlink" title="HDFS Decomission 代码解读与分析"></a>HDFS Decomission 代码解读与分析</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>目前有200+ 1.22PB HDFS3和100- 480T HDFS的节点，退役节点的速度较慢，每小时只能迁移150k左右的blocks，平均一台节点有600k左右的blocks，一台需要4小时左右。</p><h2 id="节点掉线和节点退役的区别？"><a href="#节点掉线和节点退役的区别？" class="headerlink" title="节点掉线和节点退役的区别？"></a><strong>节点掉线和节点退役的区别？</strong></h2><h3 id="节点退役"><a href="#节点退役" class="headerlink" title="节点退役"></a><strong>节点退役</strong></h3><p>首先正常将节点加入退役名单，先告诉namenode和yarn不要在往上提交新任务和写入数据了；然后等待节点上的数据块在集群中复制完成；<strong>这个时候该退役的节点是优先作为srcNode数据源的（优先选择退役中的节点作为复制数据源src，因为其无写入请求，负载低</strong>），其他节点从该退役节点复制数据块到其他节点的，所以这个时候该节点的负载会很高。所有数据块复制完毕，节点状态变成Decommissioned，可以从namenodeURL界面查看。<strong>注意数据节点退役数据开始复制的时间也是10分30s后，并不会因为是主动退役而提前开始，因为nannode和datanode永远都是维持着简单的主从关系，namenode节点不会主动向datanode节点发起任何IPC调用，datanode节点需要配合namenode完成的所有操作都是通过两者心跳应答时携带的DatanodeCommand返回的。</strong></p><h3 id="节点掉线"><a href="#节点掉线" class="headerlink" title="节点掉线"></a>节点掉线</h3><p>比如强制停止datanode，物理机挂了（比如负载高掉线，突发网络故障，硬件故障等），这些都属于节点掉线，一般默认10分30s后（主要受两个参数控制）namenode会检测到该节点通信异常掉线。然后namenode根据该节点的ip，查出该节点所有的blockid，以及对应副本所在机器，通过心跳机制安排数据复制，这时候数据的复制，数据源不在是掉线节点，而是多个副本之一所在的节点，同样这时候副本复制也遵循机架感知，副本搁置策略。</p><aside>💡 节点掉线和退役两者的区别不仅是数据的复制方式不同，还有就是namenode对Under-Replicated Blocks 的数据复制策略也是不一样的（数据块block复制的等级分成5种）；极端的例子比如单副本节点退役数据不会丢失，单副本节点掉线则会数据真的丢失；</aside><h3 id="节点掉线导致的网络风暴"><a href="#节点掉线导致的网络风暴" class="headerlink" title="节点掉线导致的网络风暴"></a>节点掉线导致的网络风暴</h3><p>几十T,甚至上百T,上百万block的节点掉线，会出现大量的RPC风暴，尤其对于大规模高负载集群来说对namenode是很大的挑战，不仅影响生产性能，也会存在很大的隐患，尤其是对于带宽有限制瓶颈的集群。</p><p>一般来说namenode检测datanode是否掉线的值是10<em>3s（心跳时间）+2</em>5min（namenode检测时间，参数是：<code>dfs.namenode.heartbeat.recheck-interval</code>）&#x3D;10分30s。如果在10min30s内的时间内带宽持续打满，RPC请求延迟，datanode和namenode节点通信不畅，很容易造成其他节点的持续掉线，形成恶性循环，这种情况应该如何避免？</p><p>NameNode 维护一个复制优先级队列，对于副本不足的文件 block 按优先级排序，仅剩下一个副本的文件 block 享有最高的复制优先级。所以从这里看集群两副本的话，只要有一个block出现异常，就只剩一个副本，就是最高优先级的块复制，会风暴模式复制，控制不好很容易影响集群性能，甚至搞挂集群。所以一般不建议集群副本因子是2。</p><aside>💡 L1(最高)：有数据丢失风险的块，如：1.只有一个副本的块（尤其对于2副本的块，下线一台节点）或者这些块有0个active副本；2，单副本在正在退役的节点拥有的块。<p>L2:block副本实际值远低于配置的值（比如3副本，缺失2个），即副本数不到期望值1&#x2F;3时的块，这些block会被第二优先级复制。比如4副本的block,有3个丢失或者坏了,它就会比4副本block丢失2个的优先复制。</p><p>L3:副本不足没有优先级L2高的那些副本，优先复制。第三优先级。</p><p>L4:block满足要求的最小副本数。副本度需求度比L2-L3都低。</p><p>L5:已损坏的块，并且当前有可用的非损坏副本</p></aside><h3 id="参数控制节点掉线导致的RPC风暴"><a href="#参数控制节点掉线导致的RPC风暴" class="headerlink" title="参数控制节点掉线导致的RPC风暴"></a>参数控制节点掉线导致的RPC风暴</h3><p>三个参数都是<code>hdfs-site.xml</code>中参数，具体可以参考apache hadoop官网，其实块的复制速度有两个方面决定，一是namenode分发任务的速度，二则是datanode之间进行复制的速度。前者可以理解成入口，后者可以当成出口。</p><blockquote><p>这些参数在hadoop2.7.3中并没有</p></blockquote><h4 id="入口参数"><a href="#入口参数" class="headerlink" title="入口参数"></a>入口参数</h4><p>从namenode层面控制任务分发，这个参数修改必须<strong>重启namenode</strong>，不需要重启datanode.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">dfs.namenode.replication.work.multiplier.per.iteration <br>这个参数apache hadoop默认值2，cdh集群默认值10<br></code></pre></td></tr></table></figure><p>这个参数决定了当NN与DN进行心跳（3s）发送任务列表时，告诉每个DN可以进行复制的block数量。比如集群有500个节点，这个值设置为10，那么一次心跳namnode可以发送datanode复制的数据块数量是10*500&#x3D;5000块。</p><p>假如一个节点掉线&#x2F;退役有800000块block需要复制，则namenode需要多长时间可以将待复制块的任务分发完给datanode呢。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">极限计算的结果：<br>任务分发时间=待复制block总数/(集群活跃dn*参数值)*心跳时间<br>time=800000/(500*10)=160次心跳*3s/每次心跳=480s=8分钟<br><br>所以节点越多，会分发任务越快,分发速度跟节点数和这个参数都成正比<br></code></pre></td></tr></table></figure><h4 id="出口参数"><a href="#出口参数" class="headerlink" title="出口参数"></a>出口参数</h4><p>相比上面从nanode任务分发控制，下面两个使用datanode层面控制，这两个参数也需要<strong>重启namenode</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">dfs.namenode.replication.max-streams<br><br>apache hadoop默认值是2，cdh集群默认20。<br></code></pre></td></tr></table></figure><p>这个参数含义是控制datanode节点进行数据复制的最大线程数，从上面我们知道block的复制优先级分成5种。这个参数控制不包含最高优先级的块复制。指除了最高优先级以外的复制流限制。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">dfs.namenode.replication.max-streams-hard-limit<br><br>这个值apache hadoop默认值2，cdh集群默认值40<br></code></pre></td></tr></table></figure><p>这个参数含义是控制datanode所有优先级块复制的流个数，包含最高优先级；一般上面和上面两个参数互相的配合使用。</p><aside>💡 前者参数控制datanode接受任务的频率，后者这两个参数进一步限制 DataNode 一次完成的最大并行线程网络传输量。具体上面参数的值设定的多少，取决于集群的规模和集群的配置，不能同一而论。一般来说从入口控制比较简单容易些。比如规模500台集群，dfs.namenode.replication.work.multiplier.per.iteration=10，那么集群一次心跳分发5000个block的量，假如集群文件存储全部打散在500台节点，每个节点同时复制10个block（实际会因为副本搁置策略，机架感知等并不会所有的节点都参与数据复制）,每个block大小128Mb,则每个节点的网络负载是128*10/3=546Mb/s，那这时候你就要看下结合实际会不会有带宽瓶颈，这么大的网络IO会不会影响正常任务的计算，如果有的话，这个值就要调小点。</aside><h2 id="如何快速下线"><a href="#如何快速下线" class="headerlink" title="如何快速下线"></a>如何快速下线</h2><p>如何让节点快速下线的本质其实就是提高副本的复制速度。主要还是上面三个参数控制.第一是控制namenode任务分发，其次控制datanode复制速率，前提是不影响正常生产任务的进行。集群规模越小，下线的越慢，比如因为分发的总数会慢很多。</p><p>比如一个500台的节点 <code>dfs.namenode.replication.work.multiplier.per.iteration=10</code>，一个50台的节点这个值要设置成100，两者namenode任务分发的速度才可以一致。具体结合实际集群规模设置。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs json">&lt;property&gt;<br>    &lt;name&gt;dfs.namenode.replication.max-streams&lt;/name&gt;<br>    &lt;value&gt;<span class="hljs-number">10</span>&lt;/value&gt;<br>&lt;/property&gt;<br>&lt;property&gt;<br>    &lt;name&gt;dfs.namenode.replication.max-streams-hard-limit&lt;/name&gt;<br>    &lt;value&gt;<span class="hljs-number">20</span>&lt;/value&gt;<br>&lt;/property&gt;<br>&lt;property&gt;<br>    &lt;name&gt;dfs.namenode.replication.work.multiplier.per.iteration&lt;/name&gt;<br>    &lt;value&gt;<span class="hljs-number">5</span>&lt;/value&gt;<br>&lt;/property&gt;<br></code></pre></td></tr></table></figure><p>在测试集群中，15台HDFS，平均每台80k blocks，每小时能迁移63k blocks，比最初的12k加速了5倍。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs json">## cdh 配置<br>dfs.namenode.replication.work.multiplier.per.iteration=<span class="hljs-number">4</span><br>dfs.namenode.replication.max-streams=<span class="hljs-number">20</span>   <br>dfs.namenode.replication.max-streams-hard-limit=<span class="hljs-number">40</span><br></code></pre></td></tr></table></figure><h4 id="影响"><a href="#影响" class="headerlink" title="影响"></a>影响</h4><p>会显著提高CPU Load, CPU IO Wait, Disk IO Wait等资源使用率</p><h3 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h3><p>当<code>dfs.hosts.exclude</code>的地址写到conf之后，就可以refreshNodes拿到exclude list并进行操作</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * Rereads conf to get hosts and exclude list file names.</span><br><span class="hljs-comment"> * Rereads the files to update the hosts and exclude lists.  It</span><br><span class="hljs-comment"> * checks if any of the hosts have changed states:</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">refreshNodes</span><span class="hljs-params">(<span class="hljs-keyword">final</span> Configuration conf)</span> <span class="hljs-keyword">throws</span> IOException &#123;<br>  refreshHostsReader(conf);<br>  namesystem.writeLock();<br>  <span class="hljs-keyword">try</span> &#123;<br>    refreshDatanodes();<br>    countSoftwareVersions();<br>  &#125; <span class="hljs-keyword">finally</span> &#123;<br>    namesystem.writeUnlock();<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>通过<code>dfs.exclude</code>读到node地址，然后标记为decommission，并使用monitor去tracking Node</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs bash">public void startDecommission(DatanodeDescriptor node) &#123;<br>  <span class="hljs-keyword">if</span> (!node.isDecommissionInProgress() &amp;&amp; !node.isDecommissioned()) &#123;<br>    // Update DN stats maintained by HeartbeatManager<br>    hbManager.startDecommission(node);<br>    // hbManager.startDecommission will <span class="hljs-built_in">set</span> dead node to decommissioned.<br>    <span class="hljs-keyword">if</span> (node.isDecommissionInProgress()) &#123;<br>      <span class="hljs-keyword">for</span> (DatanodeStorageInfo storage : node.getStorageInfos()) &#123;<br>        LOG.info(<span class="hljs-string">&quot;Starting decommission of &#123;&#125; &#123;&#125; with &#123;&#125; blocks&quot;</span>,<br>            node, storage, storage.numBlocks());<br>      &#125;<br>      node.getLeavingServiceStatus().setStartTime(monotonicNow());<br>// 开始监控这个node，并把node加入pendingNodes里面<br>      monitor.startTrackingNode(node);<br>    &#125;<br>  &#125; <span class="hljs-keyword">else</span> &#123;<br>    LOG.trace(<span class="hljs-string">&quot;startDecommission: Node &#123;&#125; in &#123;&#125;, nothing to do.&quot;</span>,<br>        node, node.getAdminState());<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>在decommission之前会先把集群的stats进行预减，比如<code>capacityUsed</code>和<br><code>capacityTotal</code> 等等</p><p>monitor的run()中会调用processPendingNodes()，从<code>pendingNodes</code>中拿出节点并进行decommission</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * Move any pending nodes into outOfServiceNodeBlocks to initiate the</span><br><span class="hljs-comment"> * decommission or maintenance mode process.</span><br><span class="hljs-comment"> *</span><br><span class="hljs-comment"> * This method must be executed under the namenode write lock to prevent</span><br><span class="hljs-comment"> * the pendingNodes list from being modified externally.</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">private</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">processPendingNodes</span><span class="hljs-params">()</span> &#123;<br>  <span class="hljs-keyword">while</span> (!pendingNodes.isEmpty() &amp;&amp;<br>      (maxConcurrentTrackedNodes == <span class="hljs-number">0</span> ||<br>          outOfServiceNodeBlocks.size() &lt; maxConcurrentTrackedNodes)) &#123;<br>    outOfServiceNodeBlocks.put(pendingNodes.poll(), <span class="hljs-literal">null</span>);<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p> 当拿到nodes之后会调用check()方法，会获取这些nodes的block信息并处理这些pendingReplication Blocks</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">private</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">check</span><span class="hljs-params">()</span> &#123;<br>  <span class="hljs-keyword">final</span> List&lt;DatanodeDescriptor&gt; toRemove = <span class="hljs-keyword">new</span> <span class="hljs-title class_">ArrayList</span>&lt;&gt;();<br><br>  <span class="hljs-keyword">if</span> (outOfServiceNodeBlocks.size() == <span class="hljs-number">0</span>) &#123;<br>    <span class="hljs-comment">// No nodes currently being tracked so simply return</span><br>    <span class="hljs-keyword">return</span>;<br>  &#125;<br><br>  <span class="hljs-comment">// Check if there are any pending nodes to process, ie those where the</span><br>  <span class="hljs-comment">// storage has not been scanned yet. For all which are pending, scan</span><br>  <span class="hljs-comment">// the storage and load the under-replicated block list into</span><br>  <span class="hljs-comment">// outOfServiceNodeBlocks. As this does not modify any external structures</span><br>  <span class="hljs-comment">// it can be done under the namenode *read* lock, and the lock can be</span><br>  <span class="hljs-comment">// dropped between each storage on each node.</span><br>  <span class="hljs-comment">//</span><br>  <span class="hljs-comment">// TODO - This is an expensive call, depending on how many nodes are</span><br>  <span class="hljs-comment">//        to be processed, but it requires only the read lock and it will</span><br>  <span class="hljs-comment">//        be dropped and re-taken frequently. We may want to throttle this</span><br>  <span class="hljs-comment">//        to process only a few nodes per iteration.</span><br>  outOfServiceNodeBlocks.keySet()<br>      .stream()<br>      .filter(n -&gt; outOfServiceNodeBlocks.get(n) == <span class="hljs-literal">null</span>)<br>      .forEach(n -&gt; scanDatanodeStorage(n, <span class="hljs-literal">true</span>));<br><br>  processMaintenanceNodes();<br>  <span class="hljs-comment">// First check the pending replication list and remove any blocks</span><br>  <span class="hljs-comment">// which are now replicated OK. This list is constrained in size so this</span><br>  <span class="hljs-comment">// call should not be overly expensive.</span><br>  processPendingReplication();<br><br>  <span class="hljs-comment">// Now move a limited number of blocks to pending</span><br>  moveBlocksToPending();<br><br>  <span class="hljs-comment">// Check if any nodes have reached zero blocks and also update the stats</span><br>  <span class="hljs-comment">// exposed via JMX for all nodes still being processed.</span><br>  checkForCompletedNodes(toRemove);<br><br>  <span class="hljs-comment">// Finally move the nodes to their final state if they are ready.</span><br>  processCompletedNodes(toRemove);<br>&#125;<br></code></pre></td></tr></table></figure><ol><li>processPendingReplication()方法中会调用isBlockReplicatedOk()方法（核心）会判断block是否需要reconstruction，并把信息放到<code>blockManager</code>里的<code>*replication queue*</code></li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs java">blockManager.neededReconstruction.add(block,<br>    liveReplicas, num.readOnlyReplicas(),<br>    num.outOfServiceReplicas(),<br>    blockManager.getExpectedRedundancyNum(block));<br></code></pre></td></tr></table></figure><ol><li>moveBlocksToPending()方法会判断queue的长度和pending replication的limit，然后为每一个需要maintenance或者decommission的node创建一个block迭代器，然后每个block都会继续调用上面提到的isBlockReplicatedOk()方法，直到达到block复制的限制就停止并退出循环</li></ol><p>moveBlocksToPending()方法中有一段关于锁的代码很有意思</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs java">Iterator&lt;DatanodeDescriptor&gt; nodeIter =<br>    Iterables.cycle(iterators.keySet()).iterator();<br><span class="hljs-keyword">while</span> (nodeIter.hasNext()) &#123;<br>  <span class="hljs-comment">// Cycle through each node with blocks which still need processed</span><br>  <span class="hljs-type">DatanodeDescriptor</span> <span class="hljs-variable">dn</span> <span class="hljs-operator">=</span> nodeIter.next();<br>  Iterator&lt;BlockInfo&gt; blockIt = iterators.get(dn);<br>  <span class="hljs-keyword">while</span> (blockIt.hasNext()) &#123;<br>    <span class="hljs-comment">// Process the blocks for the node until we find one that needs</span><br>    <span class="hljs-comment">// replication</span><br>    <span class="hljs-keyword">if</span> (blocksProcessed &gt;= blocksPerLock) &#123;<br>      blocksProcessed = <span class="hljs-number">0</span>;<br>      namesystem.writeUnlock();<br>      namesystem.writeLock();<br>    &#125;<br>    blocksProcessed++;<br>    <span class="hljs-keyword">if</span> (nextBlockAddedToPending(blockIt, dn)) &#123;<br>      <span class="hljs-comment">// Exit the inner &quot;block&quot; loop so an iterator for the next datanode</span><br>      <span class="hljs-comment">// is used for the next block.</span><br>      pendingCount++;<br>      <span class="hljs-keyword">break</span>;<br>    &#125;<br>  &#125;<br></code></pre></td></tr></table></figure><p>它在释放写锁之后立刻又申请写锁，这是因为 <strong><code>blocksProcessed</code></strong> 变量用于追踪已处理的块数量。当处理的块数量达到预定的阈值 <strong><code>blocksPerLock</code></strong> 时，通过释放当前的写锁并再次获取写锁，可以让其他等待写锁的线程有机会执行。这有助于避免长时间持有写锁而导致其他线程被阻塞的情况发生，从而提高并发性能。</p><aside>💡<p>为什么check这个函数要先调用processPendingReplication然后调用moveBlocksToPending？不能只调用一个吗？</p><p>在某些情况下，可以只调用一个函数来处理块的复制状态，但在某些情况下，调用两个函数可能是必要的。让我们来看看为什么在一些情况下需要调用这两个函数：</p><ol><li><code>processPendingReplication</code>：<ul><li>这个函数的主要任务是处理当前处于“pending replication”状态的块，确保它们的复制数量达到所需的副本数。它通常是周期性地由调度程序调用的，以确保集群中的块保持足够的冗余度。</li><li>当调用这个函数时，它会检查当前哪些块处于挂起状态，并尝试启动复制任务来复制这些块。</li></ul></li><li><code>moveBlocksToPending</code>：<ul><li>这个函数的主要任务是在特定的情况下，例如节点进入维护模式或者下线时，将相关块移动到“pending replication”状态的队列中，以等待后续的复制任务处理。</li><li>当调用这个函数时，它会检查当前是否有节点处于维护模式或者下线，然后将相关块添加到等待复制的队列中。</li></ul></li></ol><p>为什么需要同时调用这两个函数呢？因为它们处理的问题不完全一样：</p><ul><li><strong><code>processPendingReplication</code></strong> 主要关注当前正在挂起的块，而 <strong><code>moveBlocksToPending</code></strong> 主要关注于检查节点状态变化后需要进行的块处理。</li><li>在某些情况下，可能会出现新的块需要复制，或者由于节点状态变化而需要移动一些块到等待复制的队列中，所以同时调用这两个函数可以确保处理块复制状态的全面性和及时性。</li></ul></aside><ol><li>checkForCompletedNodes方法会遍历检查节点的状态，如果block的迁移任务完成了的话，就把node添加到toRemove的list中</li><li>processCompletedNodes方法拿到toRemove的list，遍历检查然后设置状态，比如改为decommissioned，并从 <strong><code>outOfServiceNodeBlocks</code></strong> 和 <strong><code>pendingRep</code></strong> 中移除</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>CodeDive</tag>
      
      <tag>Hadoop</tag>
      
      <tag>Architecture</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
